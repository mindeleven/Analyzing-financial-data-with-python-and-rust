{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "450f0168-9ffe-4033-84ae-4144f2cedf95",
   "metadata": {},
   "source": [
    "## Caching LLM Calls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56056fcc-5800-467a-936e-2b527e62a4e5",
   "metadata": {},
   "source": [
    "*[Coding along with the Udemy Course [Advanced Retrieval Augmented Generation ](https://www.udemy.com/course/advanced-retrieval-augmented-generation/) by RÃ©mi Connesson]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b1946b3-6ced-42e0-8214-0709a28fdb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fb927d8-f66c-46c2-a847-dd7706e98980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and sent your api key to github\n"
     ]
    }
   ],
   "source": [
    "api_key = pd.read_csv(\"~/tmp/chat_gpt/agentic-design-1.txt\", sep=\" \", header=None)[0][0]\n",
    "print(\"Don't be a fool and sent your api key to github\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6dd42e90-6686-4505-9030-ee03d6f89309",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d2e7f53-a2cc-451b-a8cc-b2743e35f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msg(role, content):\n",
    "    return {'role': role, 'content': content}\n",
    "\n",
    "def system(content):\n",
    "    return _msg('system', content)\n",
    "\n",
    "def user(content):\n",
    "    return _msg('user', content)\n",
    "\n",
    "def assistant(content):\n",
    "    return _msg('assistant', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0639e761-73f7-49ab-b86d-c709b0981e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f870dd8-a63c-48ce-97fd-88c3f44bcbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Caching in software engineering is a technique used to temporarily store copies of frequently accessed data or computations in order to improve performance and efficiency. By keeping this data close to where it is needed, such as in memory, subsequent requests for the same data can be served much faster than if they had to be recalculated or fetched from a slower data source (like a database or an API).\\n\\n### Key Concepts of Caching:\\n\\n1. **Types of Cache**:\\n   - **Memory Cache**: Uses RAM'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check 1, is this thing on?\n",
    "completion = await client.chat.completions.create(\n",
    "    model = model,\n",
    "    messages = [user(\"What is caching in software engineering?\")],\n",
    "    max_tokens = 100 # limit the output to save costs; answer might get cut\n",
    ")\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a6bce09-726f-4afb-97fd-c497e0ec6c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\":\"chatcmpl-ANd0gyK9hQqePVcG5gz9tAAMAj27V\",\"choices\":[{\"finish_reason\":\"length\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"Caching in software engineering is a technique used to temporarily store copies of frequently accessed data or computations in order to improve performance and efficiency. By keeping this data close to where it is needed, such as in memory, subsequent requests for the same data can be served much faster than if they had to be recalculated or fetched from a slower data source (like a database or an API).\\\\n\\\\n### Key Concepts of Caching:\\\\n\\\\n1. **Types of Cache**:\\\\n   - **Memory Cache**: Uses RAM\",\"refusal\":null,\"role\":\"assistant\",\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1730194862,\"model\":\"gpt-4o-mini-2024-07-18\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":\"fp_f59a81427f\",\"usage\":{\"completion_tokens\":100,\"prompt_tokens\":14,\"total_tokens\":114,\"completion_tokens_details\":{\"audio_tokens\":null,\"reasoning_tokens\":0},\"prompt_tokens_details\":{\"audio_tokens\":null,\"cached_tokens\":0}}}'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.json() # json string of the completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69148077-bec6-4508-8a69-f17a44a8d105",
   "metadata": {},
   "source": [
    "### Introducing Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22862270-e84d-42e2-9a1c-c2620a7a28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/diskcache/\n",
    "# Disk Cache -- Disk and file backed persistent cache\n",
    "from diskcache import Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d3b3992-ec66-400a-9ccf-bac002b45daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = Cache(directory=\".cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890f319-13b1-4b94-a1d3-28537a1cba6f",
   "metadata": {},
   "source": [
    "### Making Caching Asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7238e9fb-e55e-4f97-b51f-16b432031019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1915eb5-432b-4f3c-80eb-71e2b48dd95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a wrapper around the cache\n",
    "# so I can call it in a way that's thread safe\n",
    "async def set_async(key, val, **kwargs): # what the hell is kwargs???\n",
    "    # await the cache.set operation\n",
    "    return await asyncio.to_thread(cache.set, key, val, **kwargs)\n",
    "\n",
    "async def get_async(key, default=None, **kwargs):\n",
    "    return await asyncio.to_thread(cache.get, key, default, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a2420-dc72-40b1-a0af-053ea47f61a6",
   "metadata": {},
   "source": [
    "### Caching LLM Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4e0217a-72c7-4856-bd6a-416997ba8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e0cef82-0235-46f7-9a67-56c9bff88284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when calling a function in Python there are different ways to arrange the oder of the arguments\n",
    "# we've to make sure that arguments are in a certain order when he create a hashkey out of them\n",
    "def make_cache_key(key_name, **kwargs):\n",
    "    kwargs_string = json.dumps(kwargs, sort_keys=True)\n",
    "    kwargs_hash = md5(kwargs_string.encode('utf-8')).hexdigest()\n",
    "    cache_key = f\"{key_name}__{kwargs_hash}\"\n",
    "    return cache_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a83d2043-b7de-437d-a6e6-f778f2fb950c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demo_cache__ac6b59f8b9221cc50603ef2f4fcbf866'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_cache_key(\"demo_cache\", a=1, b=2, c=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb93f01c-a8b4-48ae-83bd-d60a8a95eb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demo_cache__ac6b59f8b9221cc50603ef2f4fcbf866'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_cache_key(\"demo_cache\", a=1, c=4, b=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc8d20-2e48-4c0a-9bc9-b736ff3391f0",
   "metadata": {},
   "source": [
    "#### __Caching an LLM Call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4bad8eca-18b4-472f-90ca-fc0493674875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching a chat completion\n",
    "# the * at the position of the first parameter forces us to explicitly pass parameters with the variable name\n",
    "# Positional-Only Arguments: When you use a single * by itself in the function signature, it indicates that all arguments \n",
    "# following the * must be passed as keyword arguments. This is useful for enforcing readability, as it makes certain \n",
    "# arguments require a name when the function is called.\n",
    "def _make_cache_key_for_chat_completion(\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    **kwargs\n",
    "):\n",
    "    return make_cache_key(\n",
    "        \"openai_chat_completion\",\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1df23345-5ff5-4c82-af31-fa2911659f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what I want to return is the chat completion\n",
    "# https://platform.openai.com/docs/guides/text-generation\n",
    "from openai.types.chat import ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e504b926-59af-459c-880b-26a53f7f050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sentinel value is often used in function parameters to indicate that no value was provided by the user. \n",
    "# a sentinel is a value you can't accidentally create\n",
    "# =>>> a trick to do this in Python is creating an object that has one memory address\n",
    "# A common sentinel for this purpose is None.\n",
    "CACHE_MISS_SENTINEL = object() # creating a sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c76119f8-b1dc-4b27-b1b7-e098f9f59be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cached_chat_completion (\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    **kwargs\n",
    ") -> ChatCompletion:\n",
    "    # 1) CREATE CACHE KEY\n",
    "    cache_key = _make_cache_key_for_chat_completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    cached_value = await get_async(cache_key, default=CACHE_MISS_SENTINEL)\n",
    "\n",
    "    # we want to return the same value out of the cache like we get from the not cached call to completition\n",
    "    # which is an object of type ChatCompletion\n",
    "\n",
    "    # 2) CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "        # no cached value so we cache the api call\n",
    "        # api call the ChatCompletion\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        # we're caching/storing the json string of the completion\n",
    "        await set_async(cache_key, completion.json())\n",
    "        return completion\n",
    "    # 3) CACHE HIT\n",
    "    else:\n",
    "        # return cached_value\n",
    "        # we want to return the same value out of the cache like we get from the not cached call to completition\n",
    "        # which is an object of type ChatCompletion\n",
    "        return ChatCompletion.validate(json.loads(cached_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d17536d-442c-4ba2-bf85-635b1011c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for demonstration purposes:\n",
    "# ChatCompletion.validate(json.loads(completion.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bf039554-f28a-4517-88d4-4e4e683cab4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = await cached_chat_completion(\n",
    "    model = model,\n",
    "    messages = [user(\"What is caching in software engineering?\")],\n",
    "    max_tokens = 100 # limit the output to save costs; answer might get cut\n",
    ")\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eeb67a9d-6fbd-4852-a973-e01fe489ebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling it once more\n",
    "completion = await cached_chat_completion(\n",
    "    model = model,\n",
    "    messages = [user(\"What is caching in software engineering?\")],\n",
    "    max_tokens = 100 # limit the output to save costs; answer might get cut\n",
    ")\n",
    "# if it's the same object from the cache the chat id should be the same\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a46a3e3-ed98-4b22-a9b5-1045550dad33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-ANd0hNO9CuzSpXrh5r8fPZyVcf6Bc', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the technique of storing copies of frequently accessed data or computational results in a temporary storage area, known as a cache, to improve the performance and efficiency of data retrieval operations. By storing this data closer to the location where it will be used (for instance, in memory rather than on disk), caching reduces the time it takes to access the data and can significantly speed up application performance.\\n\\nHere are some key aspects of caching:\\n\\n1. **Types of Caches**:\\n  ', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194863, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# direct call without cache\n",
    "completion = await client.chat.completions.create(\n",
    "    model = model,\n",
    "    messages = [user(\"What is caching in software engineering?\")],\n",
    "    max_tokens = 100 # limit the output to save costs; answer might get cut\n",
    ")\n",
    "completion # different cache id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59f27f65-2a27-4559-9dc8-9385ceee5f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "# let's loop just to make sure\n",
    "for _ in range(5):\n",
    "    completion = await cached_chat_completion(\n",
    "        model = model,\n",
    "        messages = [user(\"What is caching in software engineering?\")],\n",
    "        max_tokens = 100 # limit the output to save costs; answer might get cut\n",
    "    )\n",
    "    # if it's the same object from the cache the chat id should be the same\n",
    "    print(completion) # n times same id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1322d9b-9da2-4dfa-a320-e25c8755cbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
