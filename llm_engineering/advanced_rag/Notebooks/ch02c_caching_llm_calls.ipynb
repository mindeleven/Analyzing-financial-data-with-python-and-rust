{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "450f0168-9ffe-4033-84ae-4144f2cedf95",
   "metadata": {},
   "source": [
    "## Caching LLM Calls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56056fcc-5800-467a-936e-2b527e62a4e5",
   "metadata": {},
   "source": [
    "*[Coding along with the Udemy Course [Advanced Retrieval Augmented Generation ](https://www.udemy.com/course/advanced-retrieval-augmented-generation/) by RÃ©mi Connesson]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b1946b3-6ced-42e0-8214-0709a28fdb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fb927d8-f66c-46c2-a847-dd7706e98980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and sent your api key to github\n"
     ]
    }
   ],
   "source": [
    "api_key = pd.read_csv(\"~/tmp/chat_gpt/agentic-design-1.txt\", sep=\" \", header=None)[0][0]\n",
    "print(\"Don't be a fool and sent your api key to github\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd42e90-6686-4505-9030-ee03d6f89309",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2e7f53-a2cc-451b-a8cc-b2743e35f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msg(role, content):\n",
    "    return {'role': role, 'content': content}\n",
    "\n",
    "def system(content):\n",
    "    return _msg('system', content)\n",
    "\n",
    "def user(content):\n",
    "    return _msg('user', content)\n",
    "\n",
    "def assistant(content):\n",
    "    return _msg('assistant', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0639e761-73f7-49ab-b86d-c709b0981e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f870dd8-a63c-48ce-97fd-88c3f44bcbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Caching in software engineering refers to the practice of storing copies of frequently accessed data in a temporary storage area, known as a cache, to improve data retrieval performance and reduce latency. Caching can be applied in various contexts, including web development, databases, operating systems, and application design. Here are some key points about caching:\\n\\n1. **Purpose**: The primary goal of caching is to speed up data access and reduce the load on the underlying data sources or services, such as databases or remote APIs'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check 1, is this thing on?\n",
    "completion = await client.chat.completions.create(\n",
    "    model = model,\n",
    "    messages = [user(\"What is caching in software engineering?\")],\n",
    "    max_tokens = 100 # limit the output to save costs; answer might get cut\n",
    ")\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a6bce09-726f-4afb-97fd-c497e0ec6c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\":\"chatcmpl-ANcWzCIY3o2N3KpaWQx2lcbumrOmI\",\"choices\":[{\"finish_reason\":\"length\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"Caching in software engineering refers to the practice of storing copies of frequently accessed data in a temporary storage area, known as a cache, to improve data retrieval performance and reduce latency. Caching can be applied in various contexts, including web development, databases, operating systems, and application design. Here are some key points about caching:\\\\n\\\\n1. **Purpose**: The primary goal of caching is to speed up data access and reduce the load on the underlying data sources or services, such as databases or remote APIs\",\"refusal\":null,\"role\":\"assistant\",\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1730193021,\"model\":\"gpt-4o-mini-2024-07-18\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":\"fp_f59a81427f\",\"usage\":{\"completion_tokens\":100,\"prompt_tokens\":14,\"total_tokens\":114,\"completion_tokens_details\":{\"audio_tokens\":null,\"reasoning_tokens\":0},\"prompt_tokens_details\":{\"audio_tokens\":null,\"cached_tokens\":0}}}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.json() # json string of the completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69148077-bec6-4508-8a69-f17a44a8d105",
   "metadata": {},
   "source": [
    "### Introducing Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22862270-e84d-42e2-9a1c-c2620a7a28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/diskcache/\n",
    "# Disk Cache -- Disk and file backed persistent cache\n",
    "from diskcache import Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d3b3992-ec66-400a-9ccf-bac002b45daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = Cache(directory=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dde98dc7-cf8e-4fde-ae7d-68c75a7e45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache.set(\"thirteen\", \"It was a bright cold day in April, and the clocks were striking thirteen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77dd45e5-576a-41ff-bd44-25e77bed3fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a bright cold day in April, and the clocks were striking thirteen.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.get(\"thirteen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890f319-13b1-4b94-a1d3-28537a1cba6f",
   "metadata": {},
   "source": [
    "### Making Caching Asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7238e9fb-e55e-4f97-b51f-16b432031019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1915eb5-432b-4f3c-80eb-71e2b48dd95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a wrapper around the cache\n",
    "# so I can call it in a way that's thread safe\n",
    "async def set_async(key, val, **kwargs): # what the hell is kwargs???\n",
    "    # await the cache.set operation\n",
    "    return await asyncio.to_thread(cache.set, key, val, **kwargs)\n",
    "\n",
    "async def get_async(key, default=None, **kwargs):\n",
    "    return await asyncio.to_thread(cache.get, key, default, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "378a8dd6-7dc6-4098-8365-38ce73284d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a bright cold day in April, and the clocks were striking thirteen.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_async(\"thirteen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebcbdb00-4759-4249-8a3c-6cb398d80502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "print(await get_async(\"key that doesn't exist\", default=\"NOT FOUND\")) # returns None if default is not set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c770f23-81f3-4ba5-8fc4-6027a6391864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NOT FOUND', True, 'The Owls Are Not What They Seem')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    await get_async(\"key that doesn't exist\", default=\"NOT FOUND\"),\n",
    "    await set_async(\"Cooper\", \"The Owls Are Not What They Seem\"),\n",
    "    await get_async(\"Cooper\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a2420-dc72-40b1-a0af-053ea47f61a6",
   "metadata": {},
   "source": [
    "### Caching LLM Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e0217a-72c7-4856-bd6a-416997ba8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e0cef82-0235-46f7-9a67-56c9bff88284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when calling a function in Python there are different ways to arrange the oder of the arguments\n",
    "# we've to make sure that arguments are in a certain order when he create a hashkey out of them\n",
    "def make_cache_key(key_name, **kwargs):\n",
    "    kwargs_string = json.dumps(kwargs, sort_keys=True)\n",
    "    kwargs_hash = md5(kwargs_string.encode('utf-8')).hexdigest()\n",
    "    cache_key = f\"{key_name}__{kwargs_hash}\"\n",
    "    return cache_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a83d2043-b7de-437d-a6e6-f778f2fb950c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demo_cache__ac6b59f8b9221cc50603ef2f4fcbf866'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_cache_key(\"demo_cache\", a=1, b=2, c=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb93f01c-a8b4-48ae-83bd-d60a8a95eb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demo_cache__ac6b59f8b9221cc50603ef2f4fcbf866'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_cache_key(\"demo_cache\", a=1, c=4, b=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc8d20-2e48-4c0a-9bc9-b736ff3391f0",
   "metadata": {},
   "source": [
    "#### __Caching an LLM Call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bad8eca-18b4-472f-90ca-fc0493674875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching a chat completion\n",
    "# the * at the position of the first parameter forces us to explicitly pass parameters with the variable name\n",
    "# Positional-Only Arguments: When you use a single * by itself in the function signature, it indicates that all arguments \n",
    "# following the * must be passed as keyword arguments. This is useful for enforcing readability, as it makes certain \n",
    "# arguments require a name when the function is called.\n",
    "def _make_cache_key_for_chat_completion(\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    **kwargs\n",
    "):\n",
    "    return make_cache_key(\n",
    "        \"openai_chat_completion\",\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1df23345-5ff5-4c82-af31-fa2911659f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what I want to return is the chat completion\n",
    "# https://platform.openai.com/docs/guides/text-generation\n",
    "from openai.types.chat import ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e504b926-59af-459c-880b-26a53f7f050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sentinel value is often used in function parameters to indicate that no value was provided by the user. \n",
    "# a sentinel is a value you can't accidentally create\n",
    "# =>>> a trick to do this in Python is creating an object that has one memory address\n",
    "# A common sentinel for this purpose is None.\n",
    "CACHE_MISS_SENTINEL = object() # creating a sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c76119f8-b1dc-4b27-b1b7-e098f9f59be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cached_chat_completion (\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    **kwargs\n",
    ") -> ChatCompletion:\n",
    "    # 1) CREATE CACHE KEY\n",
    "    cache_key = _make_cache_key_for_chat_completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    cached_value = await get_async(cache_key, default=CACHE_MISS_SENTINEL)\n",
    "\n",
    "    # we want to return the same value out of the cache like we get from the not cached call to completition\n",
    "    # which is an object of type ChatCompletion\n",
    "\n",
    "    # 2) CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "        # no cached value so we cache the api call\n",
    "        # api call the ChatCompletion\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        # we're caching/storing the json string of the completion\n",
    "        await set_async(cache_key, completion.json())\n",
    "        return completion\n",
    "    # 3) CACHE HIT\n",
    "    else:\n",
    "        # return cached_value\n",
    "        # we want to return the same value out of the cache like we get from the not cached call to completition\n",
    "        # which is an object of type ChatCompletion\n",
    "        return ChatCompletion.validate(json.loads(cached_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d17536d-442c-4ba2-bf85-635b1011c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for demonstration purposes:\n",
    "# ChatCompletion.validate(json.loads(completion.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf039554-f28a-4517-88d4-4e4e683cab4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = await cached_chat_completion(\n",
    "    model = model,\n",
    "    messages = [user(\"What is caching in software engineering?\")],\n",
    "    max_tokens = 100 # limit the output to save costs; answer might get cut\n",
    ")\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eeb67a9d-6fbd-4852-a973-e01fe489ebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling it once more\n",
    "completion = await cached_chat_completion(\n",
    "    model = model,\n",
    "    messages = [user(\"What is caching in software engineering?\")],\n",
    "    max_tokens = 100 # limit the output to save costs; answer might get cut\n",
    ")\n",
    "# if it's the same object from the cache the chat id should be the same\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a46a3e3-ed98-4b22-a9b5-1045550dad33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-ANctMiPKFXDUDESLtsasf1Aimss9C', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering is a technique used to temporarily store copies of data in a cache (a fast storage layer) so that future requests for that data can be served more quickly. The primary goal of caching is to improve the performance of applications by reducing the time it takes to access frequently requested data and decreasing the load on backend systems (such as databases or APIs).\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Storage**: Caches can be implemented in various ways, such as in-memory', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194408, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# direct call without cache\n",
    "completion = await client.chat.completions.create(\n",
    "    model = model,\n",
    "    messages = [user(\"What is caching in software engineering?\")],\n",
    "    max_tokens = 100 # limit the output to save costs; answer might get cut\n",
    ")\n",
    "completion # different cache id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59f27f65-2a27-4559-9dc8-9385ceee5f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ANcqmVfYibrE7NzBAVJcQUnWcWxZk', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the practice of storing copies of frequently accessed data or computations in a temporary storage location, known as a cache, to improve system performance and efficiency. The basic idea is to reduce the time it takes to access data by keeping a version of that data closer to the place where it is needed, thereby minimizing the need to repeatedly retrieve it from a slower source, such as a database, disk, or external API.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Types', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730194248, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f59a81427f', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "# let's loop just to make sure\n",
    "for _ in range(5):\n",
    "    completion = await cached_chat_completion(\n",
    "        model = model,\n",
    "        messages = [user(\"What is caching in software engineering?\")],\n",
    "        max_tokens = 100 # limit the output to save costs; answer might get cut\n",
    "    )\n",
    "    # if it's the same object from the cache the chat id should be the same\n",
    "    print(completion) # n times same id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1322d9b-9da2-4dfa-a320-e25c8755cbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
