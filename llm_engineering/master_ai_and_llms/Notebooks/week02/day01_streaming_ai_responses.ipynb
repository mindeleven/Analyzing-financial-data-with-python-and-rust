{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c0e00e-e23f-4444-8826-13dc16b3644d",
   "metadata": {},
   "source": [
    "# Streaming AI Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873dfb11-5339-4ac9-b016-2cb9a0681d92",
   "metadata": {},
   "source": [
    "*[Coding along with the Udemy online course [LLM Engineering: Master AI & Large Language Models](https://www.udemy.com/course/llm-engineering-master-ai-and-large-language-models/) by Ed Donner; GitHub repo can be found at [github.com/ed-donner/llm_engineering](https://github.com/ed-donner/llm_engineering)]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f4f217-079a-47d6-8c54-eaff6707c394",
   "metadata": {},
   "source": [
    "## Implementing Real-Time LLM Output in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7506ab-5a8a-4685-b21c-d0c9b6f811b4",
   "metadata": {},
   "source": [
    "To get access to Google's Gemini API we first tried to just `poetry add google`.\n",
    "\n",
    "But, ccording to https://github.com/googleapis/google-api-python-client#installation, you need to install the google-api-python-client package:\n",
    "\n",
    "`poetry add google-api-python-client`\n",
    "\n",
    "The following issue that appeared, “No module named ‘google.generativeai’” is related to the installation and import of the google.generativeai module.\n",
    "\n",
    "Solution: ensure that you have installed the google-generativeai module correctly using `poetry add google-generativeai`.\n",
    "\n",
    "It's highly likely that `google` and `google-api-python-client` aren't necessary and can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49cbabce-b0f5-4a7c-82bf-d2611e010f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import google.generativeai\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa725db-9468-46f5-adfb-5e82889dd2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and sent your api key to github\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = pd.read_csv(\"~/tmp/chat_gpt/agentic-design-1.txt\", sep=\" \", header=None)[0][0]\n",
    "print(\"Don't be a fool and sent your api key to github\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409943d1-ad88-47d9-8bde-40a03774018f",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Interlude: Importing API Keys for Claude and Gemini and Testing them Briefly</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccdcf24e-75a7-4e4e-a818-e0eb9eefd0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and sent your api key to github\n"
     ]
    }
   ],
   "source": [
    "anthropic_api_key = pd.read_csv(\"~/tmp/anthropic/anthropic-key-1.txt\", sep=\" \", header=None)[0][0]\n",
    "print(\"Don't be a fool and sent your api key to github\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6d62ec-6818-4b7f-b2b5-ca72a9b878d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and sent your api key to github\n"
     ]
    }
   ],
   "source": [
    "google_api_key = pd.read_csv(\"~/tmp/google-gemini/gemini-key-1.txt\", sep=\" \", header=None)[0][0]\n",
    "print(\"Don't be a fool and sent your api key to github\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47f1eb8c-ad91-43a3-93b6-2d5c493872a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to openai\n",
    "openai = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29bada95-bab1-44dd-aa99-62352597ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to anthropic\n",
    "claude = anthropic.Anthropic(api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19549887-610a-4c35-b601-7846a0051125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " The Matthew effect refers to the sociological phenomenon of \"the rich get richer and the poor get poorer\" over time. Some key points about the Matthew effect:\n",
       "\n",
       "- It derives its name from a passage in the Biblical Gospel of Matthew: \"For to all those who have, more will be given, and they will have an abundance; but from those who have nothing, even what they have will be taken away.\"\n",
       "\n",
       "- In sociology, it was first coined by Robert K. Merton to describe how eminent scientists tend to get more credit and recognition than less known researchers, even when their work is similar in quality and importance. Their fame brings them more influence and funding opportunities.\n",
       "\n",
       "- It describes the dynamics of inequality and how small advantages accrue over time into large disparities. Those who start with more capital, skills, resources etc. are able to leverage them to gain more over time, widening the gap.\n",
       "\n",
       "- It operates across many domains beyond academia as well, including economic, socioeconomic, and health disparities between demographic groups in society. Initial advantages allow certain groups to gain more wealth, opportunities, and wellbeing over time.\n",
       "\n",
       "In essence, the Matthew effect points to the self-perpetuating nature of advantages and disadvantages in society, and how this can lead to increasing inequality and social gaps if systemic factors do not counterbalance these dynamics. Many policy interventions aim to address aspects of the Matthew effect in societies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clause example from https://www.datacamp.com/tutorial/getting-started-with-claude-3-and-the-claude-3-api\n",
    "from anthropic import HUMAN_PROMPT, AI_PROMPT\n",
    "\n",
    "completion = claude.completions.create(\n",
    "    model=\"claude-2.1\",\n",
    "    max_tokens_to_sample=300,\n",
    "    prompt=f\"{HUMAN_PROMPT} What is Matthew effect? {AI_PROMPT}\",\n",
    ")\n",
    "Markdown(completion.completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bab07ae1-cdd1-46d2-9a53-d9eff2a5d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to gemini\n",
    "google.generativeai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7299662-a4aa-4913-8b0e-d930f44dbccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten-year-old Finn was a master of lost and found. He wasn't particularly good at *finding* things, but he was exceptional at *losing* them. It was an unfortunate trait for a boy whose family was about to embark on a month-long road trip across the American Southwest.\n",
      "\n",
      "His mother, exasperated, had bought him a brand-new backpack. It was a bright, cheery orange with a worn leather flap that held a single, brass lock. \"This is special, Finn,\" she said, handing it to him. \"It’s the last one they had, and it'll keep all your things safe.”\n",
      "\n",
      "Finn wasn't interested in the leather or the brass. He was only interested in the giant, cartoon-style dinosaur emblazoned on the front. He ignored his mom's warnings about its “specialness” and stuffed it with the usual collection of forgotten toys, half-eaten snacks, and a leaky water bottle. \n",
      "\n",
      "The first day of the trip was a blur of dusty highways and endless vistas. That night, in a campground outside a sleepy town, Finn sat by the campfire, his new backpack at his feet. He was bored. He missed his friends, his video games, and the comfort of his own room. \n",
      "\n",
      "He idly kicked the backpack. It felt heavier than it should be. He unzipped it and peered inside. He swore he saw something move. He reached in, and to his surprise, his hand brushed against a smooth, cold surface. He pulled out a small, silver key.\n",
      "\n",
      "It fit perfectly into the brass lock on the flap. He unlocked it, and a plume of shimmering smoke billowed out. The smoke coalesced into a tiny, mischievous-looking goblin with a mischievous grin and pointy ears. \n",
      "\n",
      "“Greetings, Master Finn,” the goblin said, his voice a tinkling bell. “I am Flik, and this is your magical backpack.”\n",
      "\n",
      "Finn stared at the goblin, then at the backpack, then back at the goblin, speechless.\n",
      "\n",
      "“This backpack,” Flik explained, \"is a portal to any place you can imagine. You simply have to think of a place, close your eyes, and wish to be there. It will take you, but only for a few hours.”\n",
      "\n",
      "Finn’s eyes widened. This was the most amazing thing he’d ever heard. He could go anywhere, do anything! He closed his eyes, wished for a tropical beach, and… nothing happened.\n",
      "\n",
      "Flik chuckled. \"You must be more specific, Master Finn. Think of a beach, not just any beach. Picture the sand, the waves, the sound of the ocean. Where is it?\"\n",
      "\n",
      "Finn pictured a beach he’d seen in a travel brochure. “The one with the palm trees and the clear blue water, the one in Hawaii!” \n",
      "\n",
      "Flik winked. “There you go! Now close your eyes and wish!”\n",
      "\n",
      "Finn closed his eyes, wished with all his might, and suddenly, the backpack felt like it was rising. He opened his eyes to find himself standing on a beach, the sun warm on his face, the smell of salt air filling his lungs. He spent the next two hours playing in the surf, building sandcastles, and eating fresh pineapple.\n",
      "\n",
      "He realized then that this wasn't just a magical backpack; it was a portal to adventure. He could explore the world, one wish at a time, without even leaving the comfort of his family's van. He could be a pirate in the Caribbean, a knight in medieval England, a space explorer on Mars. He could even be a superhero in his own backyard, if he wanted.\n",
      "\n",
      "As the hours flew by, Finn learned to use the backpack with increasing skill. He explored ancient pyramids, went on safari in Africa, even flew on the back of a dragon. He was no longer bored. He was no longer lost. He was a world traveler, a storyteller, a master of his own destiny. \n",
      "\n",
      "He finally realized that the most important things he had found weren’t in the world outside his backpack, but within himself. He had found a love of learning, a thirst for adventure, and a newfound appreciation for the world around him. He had found his own magic, and it had nothing to do with a backpack, but with his own curious and adventurous heart.\n",
      "\n",
      "And as for his mother's warnings about the backpack's \"specialness\"? He decided to keep those to himself. The magic was his secret, and it was a secret he intended to keep, forever. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a first google request\n",
    "# https://ai.google.dev/gemini-api/docs/quickstart?lang=python\n",
    "gem_model = google.generativeai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = gem_model.generate_content(\"Write a story about a magic backpack.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2d656-896a-4721-9789-7c31c5370521",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Interlude End, let's continue with the course</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f7ca66-1d3f-48fc-a8bd-6837d8a0ddda",
   "metadata": {},
   "source": [
    "## Comparing LLMs: Let's ask them to tell a joke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527cc94-2360-40ae-ac47-95dcfa4fa557",
   "metadata": {},
   "source": [
    "__In general we pass the following information to the API:__\n",
    "\n",
    "- The __name of the model__ that should be used\n",
    "- A __system message__ that gives `overall context` for the role the LLM is playing\n",
    "- A __user message__ that provides the `actual prompt`\n",
    "\n",
    "Another parameters that can be used is __temperature__ (a number between 0 and 1). The higher the temperature, the more random the `output``\n",
    ", the lower the more focused and deterministic the output will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "819f5cc2-0516-4fdc-8a97-919be8dd5ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic messages for the \"tell a joke\" example\n",
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "869b2010-0989-43f2-befc-20ad2447d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting prompts into a list\n",
    "prompts = [\n",
    "    { \"role\": \"system\", \"content\": system_message },\n",
    "    { \"role\": \"user\", \"content\": user_prompt },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae21c5-2558-4074-9e93-2310bb5d975d",
   "metadata": {},
   "source": [
    "### OpenAI's GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22b64157-30f2-4fc7-a61c-b40648a31e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with their calculator? \n",
      "\n",
      "Because it couldn't handle their complex relationship!\n"
     ]
    }
   ],
   "source": [
    "# OpenAI's GPT model using the outdated GPT-3.5-Turbo\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "541e20e9-b43d-4785-9d00-9d15bdeef131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they wanted to reach new heights in their analysis!\n"
     ]
    }
   ],
   "source": [
    "# OpenAI's GPT model using GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f56f969b-61d8-47dc-b17b-d44950f40593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the drinks were on the house, and they wanted to elevate their data points!\n"
     ]
    }
   ],
   "source": [
    "# OpenAI's GPT model using GPT-4o\n",
    "# Temperature setting controls creativity\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fd76e5b-ecdd-485c-a36e-1c65d38f8267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists love nature hikes?\n",
      "\n",
      "Because they can't resist a good random forest!\n"
     ]
    }
   ],
   "source": [
    "# OpenAI's GPT model using GPT-4o\n",
    "# Temperature set to 1 for max creativity\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a2c198-3cc6-4b6b-892d-61494ac49f1e",
   "metadata": {},
   "source": [
    "### Anthropic's Claude model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31679d3c-545c-4aa5-a3fe-4406e10ec3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "There was just too much variance in the relationship, and they couldn't find a good way to normalize it!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1580274-e674-42a4-996e-240024100454",
   "metadata": {},
   "source": [
    "#### __Prompting Claude with streaming back results:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a405307-a1f5-4aff-9f44-80fab9aa0a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "Because there was no significant correlation between them!\n",
      "\n",
      "Ba dum tss! 🥁\n",
      "\n",
      "This joke plays on the statistical concept of \"significant correlation\" that data scientists often deal with in their work, while also making a pun about relationships. It's a bit nerdy, but should get a chuckle from a data-savvy audience!"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "# now let's stream back results\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0779ffa-4c4e-4206-b1b9-88fd687efa87",
   "metadata": {},
   "source": [
    "### Google's Gemini model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b07fe85-2d64-4fe7-84e3-edebf3021da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician? \n",
      "\n",
      "Because they couldn't see eye to eye on the p-value! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure\n",
    "# first putting in system_message\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "# second putting in the user_prompt\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d7ca1-6fcb-40e2-a9a6-defec0673e05",
   "metadata": {},
   "source": [
    "## Back to the Business Solution question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33ffea-0f95-4378-9532-58e0b3524520",
   "metadata": {},
   "source": [
    "### OpenAI's gpt-4o-mini model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f00b86cf-3d7e-42ca-a880-50fc1b4ba6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution?\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c939606-8b4d-432a-bf43-25e23a9b99f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Determining whether a business problem is suitable for a Large Language Model (LLM) solution involves several considerations. LLMs are powerful tools capable of understanding and generating human language, but they are not suitable for all problems. Here’s a guide to help you assess if an LLM is the right fit for your business problem:\n",
       "\n",
       "1. **Nature of the Problem**:\n",
       "   - **Language-based**: LLMs excel at tasks involving natural language, such as text generation, summarization, translation, sentiment analysis, and question answering.\n",
       "   - **Complexity and Nuance**: If the problem requires understanding of complex language nuances or generating coherent and contextually relevant text, an LLM might be appropriate.\n",
       "\n",
       "2. **Data Availability**:\n",
       "   - **Quality and Volume**: Ensure you have access to large volumes of high-quality text data relevant to the problem. LLMs require significant data to learn and perform effectively.\n",
       "   - **Diversity**: The data should cover a diverse range of scenarios and contexts to train the model adequately.\n",
       "\n",
       "3. **Performance Requirements**:\n",
       "   - **Accuracy and Precision**: Consider whether the level of accuracy and precision required by your business problem can be achieved with an LLM.\n",
       "   - **Latency and Real-time Processing**: Assess whether the response time of LLMs aligns with your application’s requirements, as they can be computationally intensive.\n",
       "\n",
       "4. **Ethical and Compliance Considerations**:\n",
       "   - **Bias and Fairness**: LLMs can inadvertently learn biases present in the training data. Evaluate if this poses a risk to your application.\n",
       "   - **Privacy and Security**: Ensure that using an LLM complies with data privacy regulations and security standards relevant to your industry.\n",
       "\n",
       "5. **Integration and Scalability**:\n",
       "   - **Technical Infrastructure**: Determine if your existing infrastructure can support the deployment and scaling of an LLM solution.\n",
       "   - **Interoperability**: Consider how easily the LLM can integrate with your existing systems and workflows.\n",
       "\n",
       "6. **Cost and Resource Constraints**:\n",
       "   - **Development and Maintenance Cost**: LLMs can be resource-intensive. Evaluate the cost of development, deployment, and ongoing maintenance.\n",
       "   - **Computational Resources**: Ensure you have the necessary computational resources to train, fine-tune, and deploy LLMs.\n",
       "\n",
       "7. **Alternative Solutions**:\n",
       "   - **Rule-based Systems or Simpler Models**: Sometimes, simpler models or rule-based systems might suffice, especially for well-defined tasks.\n",
       "   - **Custom Models**: Explore if a custom-built solution would be more effective for specific problems.\n",
       "\n",
       "8. **Potential for Innovation and Improvement**:\n",
       "   - **Value Addition**: Assess whether an LLM can significantly improve existing processes or create new opportunities for innovation within your business.\n",
       "\n",
       "9. **User Experience**:\n",
       "   - **Understandability and Transparency**: Consider if the LLM’s outputs can be easily understood and trusted by end-users.\n",
       "\n",
       "By carefully evaluating these factors, you can determine whether an LLM solution is suitable for your business problem. Remember that while LLMs are powerful, they are not a one-size-fits-all solution and should be used where they add the most value."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# have it stream back results in markdown\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
