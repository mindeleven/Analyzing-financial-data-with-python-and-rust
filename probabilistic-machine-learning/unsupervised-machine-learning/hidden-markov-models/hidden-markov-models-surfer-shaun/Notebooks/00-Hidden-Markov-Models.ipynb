{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3521ceeb-e9f2-43d6-ad94-daa3d47a7ea1",
   "metadata": {},
   "source": [
    "## Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687d6ee-cd3f-47a2-81b0-42425725c483",
   "metadata": {},
   "source": [
    "### Fundamental Categories of Unsupervised Learning Models\n",
    "\n",
    "#### __1. Probabilistic Clustering__\n",
    "Models in this category assign data points to clusters based on probability distributions, allowing for soft assignments where points can partially belong to multiple clusters. These models often assume the data is generated from a mixture of underlying probability distributions.\n",
    "\n",
    "> __Example - Hidden Markov Model (HMM) with Gaussian emissions:__\n",
    "> This model combines sequential state transitions with Gaussian probability distributions for observations. Each hidden state emits observable data following a Gaussian distribution, making it particularly useful for time-series data where observations depend on unobservable states. The Gaussian emissions component means each state generates data points according to a normal distribution.\n",
    "\n",
    "#### __2. Exclusive Clustering__\n",
    "These algorithms assign each data point to exactly one cluster, creating hard boundaries between groups. They typically optimize some objective function that measures cluster quality, such as minimizing within-cluster variance.\n",
    "\n",
    "> __Example - K-means Clustering:__\n",
    "> K-means iteratively assigns points to their nearest cluster center and updates these centers based on the mean of assigned points. It creates spherical clusters by minimizing the sum of squared distances between points and their assigned cluster centers. Each data point belongs exclusively to one cluster, making the boundaries between clusters clear and distinct.\n",
    "\n",
    "#### __3. Dimensionality Reduction__\n",
    "These techniques transform high-dimensional data into a lower-dimensional representation while preserving important patterns and relationships. They help address the curse of dimensionality and can reveal underlying structure in the data.\n",
    "\n",
    ">__Example - Principal Component Analysis (PCA):__\n",
    "> PCA finds orthogonal directions (principal components) in the data space that capture maximum variance. It projects data onto these components, creating a lower-dimensional representation that preserves as much variance as possible. The first principal component captures the direction of greatest variance, the second captures the next greatest variance orthogonal to the first, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7fe59-42da-4b0d-b1d1-f857799f763e",
   "metadata": {},
   "source": [
    "#### __What is the \"curse of dimensionality\"?__\n",
    "\n",
    "The curse of dimensionality refers to various challenges and counterintuitive phenomena that emerge when working with data in high-dimensional spaces. Let me break down its key aspects:\n",
    "\n",
    "1. Sparsity of Data\n",
    "As dimensions increase, the amount of data needed to maintain the same sampling density grows exponentially. For example, if you want to sample a unit line (1D) with points 0.1 units apart, you need 10 points. For a unit square (2D) with the same density, you need 100 points. For a unit cube (3D), you need 1000 points. This exponential growth means that in high dimensions, any reasonable amount of data becomes sparse.\n",
    "\n",
    "2. Distance Metrics Become Less Meaningful\n",
    "In high dimensions, the concept of \"nearest neighbors\" becomes less useful because:\n",
    "- The ratio of the distances between the nearest and farthest neighbors approaches 1\n",
    "- All points become almost equidistant from each other\n",
    "- Euclidean distance loses its intuitive meaning\n",
    "\n",
    "3. Volume Distribution\n",
    "Most of the volume of a high-dimensional sphere is concentrated in a thin \"shell\" near its surface. This means that in high dimensions:\n",
    "- Random sampling tends to produce points that lie far from the center\n",
    "- The corners of a hypercube contain most of the volume\n",
    "- The concept of a \"center\" becomes less meaningful\n",
    "\n",
    "4. Practical Implications\n",
    "These phenomena create several challenges for machine learning:\n",
    "- Models require exponentially more training data\n",
    "- Feature selection becomes critically important\n",
    "- Many clustering algorithms become less effective\n",
    "- Nearest neighbor methods may fail to find meaningful patterns\n",
    "\n",
    "This is why dimensionality reduction techniques like PCA are so important - they help us work with lower-dimensional representations of our data where these problems are less severe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a80a0-dbe7-4c9a-b7b0-b0c9c00fb2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
