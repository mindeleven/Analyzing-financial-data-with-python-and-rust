{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2611c4da-6061-4a35-b14d-3208487e01dd",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial: Building a Simple Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f6d00-0416-4b62-950e-b19b24d615b9",
   "metadata": {},
   "source": [
    "*[Coding along with the DataCamp tutorial [PyTorch Tutorial: Building a Simple Neural Network From Scratch](https://www.datacamp.com/tutorial/pytorch-tutorial-building-a-simple-neural-network-from-scratch): Learn about the basics of PyTorch, while taking a look at a detailed background on how neural networks work. Get started with PyTorch today]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6406eb9-8c1e-4b65-9915-756ecf2f1e25",
   "metadata": {},
   "source": [
    "__Definitions and explanations as created with the [OpenAI ChatGPT](https://chatgpt.com/) application.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85943618-e82f-4045-b432-daa6846d32f2",
   "metadata": {},
   "source": [
    "## What is PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ca623-bab3-4c3f-ae2c-b77bebd04de1",
   "metadata": {},
   "source": [
    "__PyTorch is an open-source deep learning framework__ primarily used __for building and training machine learning models__, especially neural networks. Think of it as a tool that makes it easier to create and work with machine learning algorithms, similar to how libraries like React or Angular help with building web applications. Here's how you can think of it from a software developer’s perspective:\n",
    "\n",
    "### What PyTorch Is:\n",
    "1. **Python Library**: PyTorch is a Python-based library, though it also has bindings for other languages like C++. It's easy to integrate into Python projects, much like how you would import and use other Python libraries.\n",
    "2. **Tensor Computation**: PyTorch provides a way to handle large multi-dimensional arrays (tensors). Tensors are similar to arrays but are optimized for large-scale numerical computations. If you've worked with NumPy, tensors in PyTorch are a bit like NumPy arrays but with the added ability to run computations on GPUs (for faster execution).\n",
    "3. **Deep Learning Framework**: PyTorch simplifies building and training neural networks, which are mathematical models inspired by how the brain works. These networks are commonly used in tasks like image recognition, natural language processing, and recommendation systems.\n",
    "\n",
    "### What PyTorch Can Be Used For:\n",
    "1. **Building Neural Networks**: PyTorch helps in creating deep learning models by providing tools to easily define layers, activation functions, loss functions, and optimizers. You can think of it as giving you pre-made building blocks for creating complex models.\n",
    "2. **Training Models**: PyTorch automates the process of adjusting the model’s parameters based on the data it sees. This process is called \"training,\" and PyTorch makes it easy to handle complex training loops with features like automatic differentiation (Autograd), which helps calculate gradients (used for optimization).\n",
    "3. **GPU Acceleration**: PyTorch allows you to run your code on GPUs with minimal changes, significantly speeding up the training process. It abstracts away most of the complexity of dealing with GPU computations.\n",
    "4. **Research and Prototyping**: PyTorch is widely used in research because it’s flexible and dynamic. Researchers can experiment with new ideas quickly, as the framework lets you change models on the fly, similar to the way developers might modify and test code iteratively.\n",
    "5. **Production-Ready Applications**: PyTorch can also be used in production environments. Models built in PyTorch can be deployed in real-world applications, such as recommendation engines, autonomous systems, or healthcare diagnostics.\n",
    "\n",
    "### Key Concepts (for a Developer):\n",
    "- **Tensors**: Multi-dimensional arrays, like matrices or higher-dimensional data structures, which can represent anything from a single number to complex datasets.\n",
    "- **Autograd**: A system that automatically calculates gradients during backpropagation (the process of updating the model's weights), which is essential for model optimization.\n",
    "- **Neural Networks**: Layers of connected nodes (like neurons in a brain) that learn to recognize patterns in data.\n",
    "\n",
    "In short, PyTorch is a flexible and efficient tool for developing machine learning models, offering a smooth integration of Python with the power of GPU computing, making it a favorite for both researchers and engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede4b87-cc5d-462c-bf5f-b3bb65e850df",
   "metadata": {},
   "source": [
    "## What are Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb735dc1-3786-451c-ba1f-6bdd81fe678b",
   "metadata": {},
   "source": [
    "A neural network is a type of machine learning model inspired by the way the human brain works. It's composed of layers of connected \"neurons\" that process data and learn patterns in it. Here's a breakdown of the key components:\n",
    "\n",
    "- **Input Layer**:  \n",
    "  The first layer in the network. It takes in the raw data (inputs) that you want the network to process. Each neuron in this layer corresponds to a specific feature or value in the input data. For example, if you are working with an image, each pixel value could be an input.\n",
    "\n",
    "- **Hidden Layer(s)**:  \n",
    "  These are the intermediate layers between the input and output layers. A neural network can have multiple hidden layers, and they are where the real learning happens. Each neuron in a hidden layer receives inputs from the previous layer, processes them, and passes the result to the next layer. The term \"hidden\" refers to the fact that these layers aren't directly exposed to the input or output, they only exist to learn from and transform the data.\n",
    "\n",
    "- **Output Layer**:  \n",
    "  This is the final layer of the neural network, and its neurons provide the predictions or results based on the inputs processed by the hidden layers. If you're working on a classification problem (e.g., predicting whether an image is of a cat or a dog), the output layer would have one neuron for each possible class (one for \"cat,\" one for \"dog\").\n",
    "\n",
    "- **Neurons**:  \n",
    "  The basic units of a neural network. Each neuron receives inputs (from the previous layer), applies a mathematical transformation to them (usually involving an activation function), and sends the result to neurons in the next layer. Neurons are connected to each other, and the strength of these connections is determined by \"weights,\" which are adjusted during training.\n",
    "\n",
    "- **Activation Function**:  \n",
    "  A mathematical function applied to the output of each neuron. It determines whether a neuron should be activated or not (whether its output should be passed to the next layer). Activation functions introduce non-linearity into the model, which allows neural networks to learn complex patterns. Common activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh.\n",
    "\n",
    "### Putting It Together:\n",
    "1. Data enters through the **input layer**.\n",
    "2. The data is passed to one or more **hidden layers** of **neurons**, where it's transformed and processed using **activation functions**.\n",
    "3. Finally, the processed data reaches the **output layer**, which gives the final prediction or result.\n",
    "\n",
    "In essence, a neural network is a series of transformations applied to data, where the model learns by adjusting the connections (weights) between neurons in response to the patterns it finds in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41cdec-52dd-4f42-9d15-3177e0c19bd1",
   "metadata": {},
   "source": [
    "## Weight initialization in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128319a-44c5-4ca0-90d0-b9c30349daad",
   "metadata": {},
   "source": [
    "**Weight initialization** in a neural network refers to the process of setting the initial values of the weights (the parameters that connect the neurons) before the training process begins. Proper initialization is crucial because it impacts how quickly and effectively the network learns during training. If weights are not initialized well, the network can suffer from problems like slow convergence, getting stuck in local minima, or vanishing/exploding gradients. Here are some common approaches to weight initialization:\n",
    "\n",
    "### 1. **Zero Initialization**:\n",
    "   - **What it is**: All the weights are initialized to zero.\n",
    "   - **Issue**: This approach should generally be avoided because if all weights are zero, neurons in the network will receive the same gradients during backpropagation, and they will all update the same way. This makes all neurons behave identically, and the network won't learn effectively.\n",
    "   \n",
    "### 2. **Random Initialization**:\n",
    "   - **What it is**: Weights are initialized randomly, usually from a small uniform or normal distribution.\n",
    "   - **Why it helps**: Random initialization ensures that neurons start with different weights, breaking the symmetry problem of zero initialization.\n",
    "   - **Issue**: If the random values are too large, they can lead to exploding gradients; if too small, they can cause vanishing gradients, making the network hard to train.\n",
    "\n",
    "### 3. **Xavier/Glorot Initialization**:\n",
    "   - **What it is**: Weights are initialized by drawing from a distribution (either uniform or normal) with a variance that depends on the number of neurons in the previous and next layers. Specifically, the variance is set to \\( \\frac{2}{n_{\\text{in}} + n_{\\text{out}}} \\), where \\( n_{\\text{in}} \\) is the number of input neurons and \\( n_{\\text{out}} \\) is the number of output neurons.\n",
    "   - **Why it helps**: This initialization is designed to keep the scale of the gradients roughly the same across layers. It helps prevent vanishing or exploding gradients, making it suitable for networks with sigmoid or tanh activation functions.\n",
    "   - **Best for**: Sigmoid and tanh activation functions.\n",
    "\n",
    "### 4. **He/Kaiming Initialization**:\n",
    "   - **What it is**: Weights are initialized with a variance of \\( \\frac{2}{n_{\\text{in}}} \\), where \\( n_{\\text{in}} \\) is the number of input neurons. This initialization is more aggressive than Xavier and is designed for layers with ReLU activation functions, which tend to \"kill\" neurons (output zero) if weights are not initialized properly.\n",
    "   - **Why it helps**: It accounts for the ReLU function's behavior by using a higher variance to ensure that gradients do not vanish.\n",
    "   - **Best for**: ReLU and variants (like Leaky ReLU) activation functions.\n",
    "\n",
    "### Summary of When to Use:\n",
    "- **Zero Initialization**: Avoid unless in specific edge cases (like bias terms).\n",
    "- **Random Initialization**: Simple but may lead to gradient issues.\n",
    "- **Xavier/Glorot Initialization**: Good for tanh and sigmoid activations.\n",
    "- **He/Kaiming Initialization**: Best for ReLU-based activations.\n",
    "\n",
    "Proper weight initialization is key to ensuring that your neural network trains efficiently and learns effectively from data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bef37-6b04-4c19-9930-58b6f4b67e36",
   "metadata": {},
   "source": [
    "### What are *Gradients* in Neural Networks?\n",
    "\n",
    "In the context of neural networks, **gradients** refer to the partial derivatives of the loss function (which measures how far the network's predictions are from the actual values) with respect to the weights of the network. Gradients indicate the direction and rate at which the weights need to be adjusted to minimize the loss during training. They play a central role in the optimization process, particularly in the **backpropagation** algorithm.\n",
    "\n",
    "### How Gradients Work in Neural Networks:\n",
    "1. **Forward Pass**: The input data is passed through the network, layer by layer, to generate predictions. The loss function then computes how far off these predictions are from the actual target values.\n",
    "   \n",
    "2. **Backward Pass (Backpropagation)**: The gradients are calculated by computing how much a small change in each weight will affect the loss. These gradients are then propagated backward from the output layer to the input layer. This allows the network to compute how each weight in the network contributes to the error.\n",
    "\n",
    "3. **Weight Update**: Once the gradients are computed, they are used to update the weights. Typically, the weights are adjusted using **gradient descent** or one of its variants (like stochastic gradient descent). The update rule is:\n",
    "   \\[\n",
    "   \\text{New weight} = \\text{Old weight} - \\eta \\times \\text{Gradient}\n",
    "   \\]\n",
    "   where \\( \\eta \\) is the learning rate, and the gradient tells how much and in which direction to update the weights.\n",
    "\n",
    "### Gradients and Weight Initialization:\n",
    "The initial values of the weights, set by the weight initialization methods (like zero, random, Xavier, or He initialization), directly influence the gradients during the early stages of training. Here's why:\n",
    "\n",
    "- **Exploding Gradients**: If the initial weights are too large, the gradients can become very large during backpropagation. This can cause the weights to update too aggressively, leading to instability in the learning process, and the model might not converge.\n",
    "  \n",
    "- **Vanishing Gradients**: If the initial weights are too small, the gradients may become very small during backpropagation. This causes the weights to update very slowly, leading to slow learning or the network getting \"stuck\" without learning meaningful patterns, particularly in deep networks.\n",
    "\n",
    "Proper initialization methods like **Xavier/Glorot** and **He/Kaiming** are designed to maintain reasonable gradient magnitudes across layers, preventing these problems by ensuring that the weights neither shrink nor blow up as they propagate through the network. Thus, gradients are central to learning in neural networks, and weight initialization directly influences how well the gradients behave during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e37f5-8196-4e2e-878b-42930eb379ad",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2d30d-b1e8-4805-bdb7-136e064c664b",
   "metadata": {},
   "source": [
    "**Forward propagation** is the process in which input data is passed through a neural network to generate an output or prediction. It’s the first step in training and using a neural network, where data flows from the input layer, through the hidden layers, to the output layer. Here’s how it works:\n",
    "\n",
    "### How Forward Propagation Works:\n",
    "1. **Input Layer**: \n",
    "   - The process starts when the input data is fed into the network. Each feature of the input data corresponds to a neuron in the input layer. For example, in image recognition, each pixel value in the image could be an input neuron.\n",
    "   \n",
    "2. **Weighted Sum in Neurons**: \n",
    "   - Each neuron in the subsequent layers (starting from the first hidden layer) computes a weighted sum of the inputs it receives. This is calculated as:\n",
    "   - \n",
    "     \\[\n",
    "     z = \\sum (w_i \\times x_i) + b\n",
    "     \\]\n",
    "\n",
    "     **z=∑(wi​×xi​)+b**\n",
    "     \n",
    "     Where:\n",
    "     - \\( w_i \\) are the weights connecting the neurons,\n",
    "     - \\( x_i \\) are the inputs from the previous layer,\n",
    "     - \\( b \\) is the bias term (an extra parameter to shift the activation function),\n",
    "     - \\( z \\) is the result of the weighted sum.\n",
    "\n",
    "3. **Activation Function**:\n",
    "   - After computing the weighted sum \\( z \\), each neuron applies an **activation function** to introduce non-linearity to the model. The activation function determines whether a neuron should \"activate\" (output a non-zero value) based on the input. Common activation functions include:\n",
    "     - **ReLU** (Rectified Linear Unit): Outputs max⁡(0,z),\n",
    "     - **Sigmoid**: Outputs a value between 0 and 1,\n",
    "     - **Tanh**: Outputs a value between -1 and 1.\n",
    "\n",
    "4. **Hidden Layers**:\n",
    "   - The activated output of each neuron in the current layer becomes the input for the neurons in the next layer. This process continues through all the **hidden layers** in the network.\n",
    "\n",
    "5. **Output Layer**:\n",
    "   - Finally, the data reaches the output layer, where each neuron provides a prediction or result based on the activations from the last hidden layer. For example, in a classification task, the output layer might represent the probabilities for each class (e.g., dog or cat in an image classifier).\n",
    "\n",
    "### Example:\n",
    "Imagine a neural network classifying whether an image is of a cat or dog. The **input layer** receives pixel values from the image. Each pixel value is multiplied by a weight and passed to the neurons in the **hidden layers**, where the weighted sums are processed by activation functions. The final **output layer** will generate two values representing the likelihood that the image is a cat or a dog.\n",
    "\n",
    "### Key Points:\n",
    "- **Forward propagation** is the flow of information through the network from the input to the output, without modifying the weights.\n",
    "- It generates the network’s predictions, which are compared with the actual values during training.\n",
    "- It involves calculating weighted sums, applying activation functions, and passing the results to the next layer.\n",
    "\n",
    "In summary, forward propagation is how a neural network processes input data to produce an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2386035a-fe06-4aef-a197-4218a72f1090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
