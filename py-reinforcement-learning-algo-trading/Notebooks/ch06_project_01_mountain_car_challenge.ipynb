{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa1b1d9",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Project 1: The Mountain Car Challenge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1f261",
   "metadata": {},
   "source": [
    "*[Coding along with [Reinforcement Learning for Algorithmic Trading with Python](https://www.udemy.com/course/reinforcement-learning-for-algorithmic-trading-with-python/), an Udemy online course by Alexander Hagmann]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48743eb8-deae-411d-ae52-2051db0f1957",
   "metadata": {},
   "source": [
    "**From https://github.com/openai:** [Gym](https://github.com/openai/gym) is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym's API has become the field standard for doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f2355-60f6-4288-b1f9-3945e41127d2",
   "metadata": {},
   "source": [
    "**[Gymnasium](https://gymnasium.farama.org/content/basic_usage/)* is a project that provides an API for all single agent reinforcement learning environments, and includes implementations of common environments: cartpole, pendulum, mountain-car, mujoco, atari, and more.\n",
    "\n",
    "The API contains four key functions: make, reset, step and render, that this basic usage will introduce you to. At the core of Gymnasium is Env, a high-level python class representing a markov decision process (MDP) from reinforcement learning theory (this is not a perfect reconstruction, and is missing several components of MDPs). Within gymnasium, environments (MDPs) are implemented as Env classes, along with Wrappers, which provide helpful utilities and can change the results passed to the user ([Gymnasium Documentation: Basic Usage](https://gymnasium.farama.org/content/basic_usage/)).\n",
    "\n",
    "**The Gymnasium GitHub repository can be found at [https://github.com/Farama-Foundation/Gymnasium](https://github.com/Farama-Foundation/Gymnasium).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a738c-1302-40a7-a103-4d5939770ebd",
   "metadata": {},
   "source": [
    "### One random Episode - with human rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f3cb3-2edb-414e-862b-892efefa4afd",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">__Prompt:__ I have installed Anaconda and all required libraries to work with OpenAI´s gymnasium Reinforcement Learning environments. Also I have installed Microsoft Visual Studio to visualize environments such as Mountain Car. Please provide a Python code example that creates and runs a random episode in the 'MountainCar-v0' environment using the gymnasium library, with human rendering enabled and printing the total reward and number of steps at the end. Please note that gym is now gymnasium.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2592b3-7ac5-40c0-bd06-a55a590c66a8",
   "metadata": {},
   "source": [
    "Great! Here’s a Python code example that creates and runs a random episode in the **'MountainCar-v0'** environment using the **gymnasium** library, with **human rendering** enabled and printing the **total reward** and **number of steps** at the end:\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "# Initialize the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "num_steps = 0\n",
    "done = False\n",
    "\n",
    "# Run a random episode\n",
    "while not done:\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Perform the action in the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Accumulate the reward and count the steps\n",
    "    total_reward += reward\n",
    "    num_steps += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Total Steps: {num_steps}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "```\n",
    "\n",
    "### Explanation of the Code:\n",
    "1. **`gymnasium.make(\"MountainCar-v0\", render_mode=\"human\")`**: \n",
    "   - Initializes the **MountainCar-v0** environment with rendering enabled so you can visually observe the car’s movement.\n",
    "\n",
    "2. **`env.reset()`**:\n",
    "   - Resets the environment to its initial state and returns the initial observation and environment info.\n",
    "\n",
    "3. **`env.action_space.sample()`**:\n",
    "   - Randomly samples an action from the action space (which, for **MountainCar**, can be one of three discrete actions: push left, no push, or push right).\n",
    "\n",
    "4. **`env.step(action)`**:\n",
    "   - Takes the sampled action and returns the next observation, reward, whether the episode is done, and any additional info. The loop continues until `done` is `True` (when the car reaches the flag or max steps are taken).\n",
    "\n",
    "5. **`total_reward` and `num_steps`**:\n",
    "   - These accumulate the reward over the episode and count the number of steps, respectively.\n",
    "\n",
    "6. **`env.close()`**:\n",
    "   - Closes the environment to stop the rendering window after the episode finishes.\n",
    "\n",
    "### Libraries and Setup:\n",
    "- **gymnasium**: The new version of the Gym library for RL environments.\n",
    "- **render_mode='human'**: Ensures the environment window is shown for human interaction.\n",
    "  \n",
    "This code will run a random agent in the **MountainCar** environment, render the environment visually, and display the total reward and number of steps after the episode ends.\n",
    "\n",
    "Let me know if you encounter any issues or need further customizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d284051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gymnasium as gym\\n\\n# Create the environment\\n# initializing the MountainCar-v0 environment with human rendering to visualize the agent\\'s actions\\n# the environment is rendered in real-time using render_mode=\"human\"\\nenv = gym.make(\"MountainCar-v0\", render_mode=\"human\")\\n\\n# Initialize the environment\\nobservation, info = env.reset()\\n\\ntotal_reward = 0\\nnum_steps = 0\\ndone = False\\n\\n# Run a random episode\\nwhile not done:\\n    # Take a random action, like driving right or left or doing nothing\\n    action = env.action_space.sample() # selects a random action at each step\\n    \\n    # Perform the action in the environment\\n    # env.step(action) executes the action and returns the new state, reward, and whether the episode has finished\\n    # observing the result for the next step/sate (???)\\n    observation, reward, done, truncated, info = env.step(action)\\n    \\n    # Accumulate the reward and count the steps\\n    total_reward += reward\\n    num_steps += 1\\n\\n# Print the results\\nprint(f\"Total Reward: {total_reward}\")\\nprint(f\"Total Steps: {num_steps}\")\\n\\n# Close the environment\\nenv.close()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment\n",
    "# initializing the MountainCar-v0 environment with human rendering to visualize the agent's actions\n",
    "# the environment is rendered in real-time using render_mode=\"human\"\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "# Initialize the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "num_steps = 0\n",
    "done = False\n",
    "\n",
    "# Run a random episode\n",
    "while not done:\n",
    "    # Take a random action, like driving right or left or doing nothing\n",
    "    action = env.action_space.sample() # selects a random action at each step\n",
    "    \n",
    "    # Perform the action in the environment\n",
    "    # env.step(action) executes the action and returns the new state, reward, and whether the episode has finished\n",
    "    # observing the result for the next step/sate (???)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Accumulate the reward and count the steps\n",
    "    total_reward += reward\n",
    "    num_steps += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Total Steps: {num_steps}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b666f-7785-4756-bb25-6f75d1ea5777",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Could you give me please a more verbose explanations of the steps Episode Initialization and Episode Execution that describes the state of the variables in each step?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195958bb-7ca5-47fc-9e03-f67c84af89e0",
   "metadata": {},
   "source": [
    "Certainly! I'll break down the steps of **Episode Initialization** and **Episode Execution** in more detail, focusing on the state of the variables at each stage of the process.\n",
    "\n",
    "### **1. Episode Initialization**\n",
    "\n",
    "The first part of the code involves initializing the environment and setting up the necessary variables.\n",
    "\n",
    "```python\n",
    "# Create the MountainCar environment\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "# Initialize the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "num_steps = 0\n",
    "done = False\n",
    "```\n",
    "\n",
    "#### Step-by-Step Explanation:\n",
    "\n",
    "- **`env = gym.make(\"MountainCar-v0\", render_mode=\"human\")`**:\n",
    "   - This line **creates and initializes** the 'MountainCar-v0' environment.\n",
    "   - **`env`**: This variable now holds the Mountain Car environment object, which has properties and methods to interact with the simulation.\n",
    "   - **`render_mode=\"human\"`**: This parameter specifies that the environment should render to a window so you can visually see the simulation as it runs.\n",
    "\n",
    "- **`observation, info = env.reset()`**:\n",
    "   - **Resetting the environment**: The `reset()` method sets the environment to its **initial state**, ready to start a new episode.\n",
    "   - **`observation`**: This variable holds the initial state (or observation) of the environment, which is a **numpy array** representing the position and velocity of the car. For MountainCar, it’s typically a 2D array like `[-0.5, 0.0]`, where `-0.5` is the initial position and `0.0` is the initial velocity.\n",
    "   - **`info`**: This variable holds **additional information** about the environment's state or configuration (which can vary based on the environment). It's often empty or not necessary for simple tasks.\n",
    "\n",
    "- **`total_reward = 0`**:\n",
    "   - **`total_reward`** is a variable used to track the **total accumulated reward** for the current episode.\n",
    "   - In the MountainCar environment, each step usually gives a negative reward, and your goal is to minimize the number of steps to reach the flag (i.e., the top of the hill).\n",
    "   - Initial value is set to **0** because no steps have been taken yet.\n",
    "\n",
    "- **`num_steps = 0`**:\n",
    "   - **`num_steps`** tracks the **number of steps** taken in the episode. It’s initialized to **0** at the start.\n",
    "   \n",
    "- **`done = False`**:\n",
    "   - **`done`** is a boolean flag that indicates if the **episode is finished**.\n",
    "   - It starts as **False** because we haven't yet reached the goal or the max number of steps.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Episode Execution**\n",
    "\n",
    "This part of the code runs the episode, taking actions and interacting with the environment until the car reaches the goal or a maximum number of steps is reached.\n",
    "\n",
    "```python\n",
    "while not done:\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Perform the action in the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Accumulate the reward and count the steps\n",
    "    total_reward += reward\n",
    "    num_steps += 1\n",
    "```\n",
    "\n",
    "#### Step-by-Step Explanation:\n",
    "\n",
    "- **`while not done:`**:\n",
    "   - This is a **loop** that runs as long as `done` is **False**, meaning the episode hasn't yet finished.\n",
    "   - Every time we take a step in the environment, the loop checks if `done` has become `True` (i.e., the car reaches the goal or the episode exceeds a step limit).\n",
    "\n",
    "- **`action = env.action_space.sample()`**:\n",
    "   - This line randomly selects an action from the **action space** of the Mountain Car environment.\n",
    "   - **`action`** is an integer that can be **0** (push left), **1** (no push), or **2** (push right). This corresponds to the agent’s attempt to push the car left, do nothing, or push it right.\n",
    "\n",
    "- **`observation, reward, done, truncated, info = env.step(action)`**:\n",
    "   - **`env.step(action)`**: This method **performs the action** in the environment, advancing the simulation by one time step. It returns multiple pieces of information:\n",
    "     - **`observation`**: The new state of the environment after the action. In the MountainCar environment, this is an array representing the new position and velocity of the car.\n",
    "     - **`reward`**: The **reward** received from taking that action. In MountainCar, it’s typically a small negative value (e.g., -1) to encourage the agent to finish the episode quickly.\n",
    "     - **`done`**: A boolean flag indicating whether the **episode is finished**. This will become **True** if the car reaches the flag (goal) or the episode exceeds the step limit.\n",
    "     - **`truncated`**: A boolean flag indicating whether the episode was **truncated** (i.e., ended prematurely, often because a max time or step limit was reached).\n",
    "     - **`info`**: Additional information, often empty or not relevant for basic tasks.\n",
    "   \n",
    "- **`total_reward += reward`**:\n",
    "   - This line **updates the total reward** by adding the reward received from the current step. Since each step in MountainCar gives a negative reward, this value will become increasingly negative over time.\n",
    "   \n",
    "- **`num_steps += 1`**:\n",
    "   - This increments the **step counter**, tracking how many steps have been taken in the episode.\n",
    "\n",
    "### Final State (End of Episode):\n",
    "Once the `done` flag becomes `True`, meaning the episode has ended (the car reached the goal or the max number of steps was taken), the loop exits.\n",
    "\n",
    "```python\n",
    "# Print the results\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Total Steps: {num_steps}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "```\n",
    "\n",
    "- **Total Reward** and **Total Steps** are printed:\n",
    "   - **`total_reward`**: This will show the **cumulative reward** over the entire episode (typically negative for MountainCar unless you reach the goal quickly).\n",
    "   - **`num_steps`**: This shows the **number of actions** the agent took to complete the episode.\n",
    "\n",
    "- **`env.close()`**:\n",
    "   - This closes the environment and stops rendering. It’s important to call this to properly clean up resources, especially if you're using human rendering.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Variable States Throughout the Episode:\n",
    "- **`env`**: Holds the environment object, allowing interaction with the MountainCar-v0 environment.\n",
    "- **`observation`**: Initially set by `reset()` to a state like `[-0.5, 0.0]` and updated at each step to reflect the new position and velocity of the car.\n",
    "- **`total_reward`**: Starts at 0 and decreases as the episode progresses (MountainCar provides negative rewards for each step).\n",
    "- **`num_steps`**: Starts at 0 and increments with each step in the environment.\n",
    "- **`done`**: Initially `False`, becomes `True` when the car reaches the goal or the max steps are exceeded, signaling the end of the episode.\n",
    "- **`action`**: A random action (either 0, 1, or 2) sampled from the action space at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6908df2-dd87-4e59-ae9e-623c5afc848b",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">__Prompt:__ It doesn't stop automatically (unless it reaches the goal). Please adjust and add a maximum number of steps per episode.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62494aac-de96-42b5-a842-689db8c50a59",
   "metadata": {},
   "source": [
    "To ensure that the episode terminates after a certain number of steps (even if the goal is not reached), we can add a condition to break the loop after a **maximum number of steps**. Here's the modified code:\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the MountainCar environment\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "max_steps_per_episode = 200  # Define the maximum number of steps per episode\n",
    "\n",
    "# Initialize the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "num_steps = 0\n",
    "done = False\n",
    "\n",
    "# Run a random episode with a maximum number of steps\n",
    "while not done and num_steps < max_steps_per_episode:\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Perform the action in the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Accumulate the reward and count the steps\n",
    "    total_reward += reward\n",
    "    num_steps += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Total Steps: {num_steps}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "```\n",
    "\n",
    "### Changes Made:\n",
    "1. **`max_steps_per_episode = 200`**:\n",
    "   - A new variable `max_steps_per_episode` defines the **maximum number of steps** an episode can run before it is forcefully stopped.\n",
    "   - You can adjust this number based on your requirements.\n",
    "\n",
    "2. **`while not done and num_steps < max_steps_per_episode:`**:\n",
    "   - The loop now also checks if `num_steps` has reached `max_steps_per_episode`. If the maximum number of steps is reached, the episode will terminate, even if the goal hasn’t been reached.\n",
    "\n",
    "Now, the episode will automatically stop either when:\n",
    "1. The agent reaches the goal (i.e., `done == True`).\n",
    "2. The agent exceeds the maximum number of steps (200 steps in this case).\n",
    "\n",
    "Let me know if you need further adjustments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e544dce-8eb3-4577-9e94-bbc32fb0d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: -200.0\n",
      "Total Steps: 200\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create the MountainCar environment\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "max_steps_per_episode = 200  # Define the maximum number of steps per episode\n",
    "\n",
    "# Initialize the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "num_steps = 0\n",
    "done = False\n",
    "\n",
    "# Run a random episode with a maximum number of steps\n",
    "while not done and num_steps < max_steps_per_episode:\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Perform the action in the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Accumulate the reward and count the steps\n",
    "    total_reward += reward\n",
    "    num_steps += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Total Steps: {num_steps}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dbc87f-d10b-46de-bcbc-82119bb5a3db",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">__Prompt:__ Please use the max_episode_steps parameter in gym.make() and truncated.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59693c75-7ce0-4686-9297-75667bbac58c",
   "metadata": {},
   "source": [
    "Sure! To specify the maximum number of steps directly in the environment initialization, we can use the `max_episode_steps` parameter in the `gym.make()` function. Additionally, we can handle the episode termination using the **`truncated`** flag, which indicates whether the episode ended because it reached the maximum number of steps.\n",
    "\n",
    "Here’s the adjusted code:\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the MountainCar environment with max episode steps\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\", max_episode_steps=200)\n",
    "\n",
    "# Initialize the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "num_steps = 0\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "# Run a random episode\n",
    "while not done and not truncated:\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Perform the action in the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Accumulate the reward and count the steps\n",
    "    total_reward += reward\n",
    "    num_steps += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Total Steps: {num_steps}\")\n",
    "\n",
    "# Check if the episode was truncated\n",
    "if truncated:\n",
    "    print(\"Episode was truncated due to reaching the maximum number of steps.\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "```\n",
    "\n",
    "### Changes Explained:\n",
    "\n",
    "1. **`max_episode_steps=200`**:\n",
    "   - This parameter is passed to `gym.make()` to specify that each episode should end after a maximum of **200 steps**.\n",
    "   - You can adjust this number if you want more or fewer steps per episode.\n",
    "\n",
    "2. **`observation, reward, done, truncated, info = env.step(action)`**:\n",
    "   - The `step()` function returns the `truncated` flag, which is set to `True` if the episode ends because it has reached the maximum number of steps.\n",
    "   - The loop will now terminate if either `done` is `True` (the goal is reached) or `truncated` is `True` (the maximum step count is reached).\n",
    "\n",
    "3. **`if truncated:`**:\n",
    "   - After the episode ends, this check will print a message if the episode was truncated because it reached the maximum number of steps.\n",
    "\n",
    "### Explanation of Variables:\n",
    "- **`done`**: Indicates if the episode has finished because the goal was reached.\n",
    "- **`truncated`**: Indicates if the episode was **cut off** because it reached the **maximum number of steps**.\n",
    "\n",
    "This method ensures that the episode automatically stops when either the goal is reached or the maximum number of steps is hit, and it leverages the `max_episode_steps` parameter of the environment itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3fcd2c-c34b-4689-afde-faecdb58ea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: -200.0\n",
      "Total Steps: 200\n",
      "Episode was truncated due to reaching the maximum number of steps.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create the MountainCar environment with max episode steps\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\", max_episode_steps=200)\n",
    "\n",
    "# Initialize the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "num_steps = 0 # num_steps = env._elapsed_steps\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "# Run a random episode\n",
    "while not done and not truncated:\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Perform the action in the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Accumulate the reward and count the steps\n",
    "    total_reward += reward\n",
    "    num_steps += 1 # could be removed because env._elapsed_steps returns the same information\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Total Steps: {num_steps}\") # num_steps = env._elapsed_steps\n",
    "\n",
    "# Check if the episode was truncated\n",
    "if truncated:\n",
    "    print(\"Episode was truncated due to reaching the maximum number of steps.\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55a4ea-97a2-4064-a286-6b1de88e9ed8",
   "metadata": {},
   "source": [
    "#### Breaking down the code line by line (at least more or less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908e206f-3c89-4a28-abc2-627e721ffeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing gymnasium\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542451c3-4b89-4907-a3e0-b96591e6afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gymnasium.farama.org/environments/classic_control/mountain_car/\n",
    "# creating the environment for the MountainCar challenge\n",
    "# selecting the 'human' render mode\n",
    "# setting the maximum episodes to 200\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\", max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d91e81f-9c89-423a-b6cf-0334ca6c0edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<MountainCarEnv<MountainCar-v0>>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a505254-7b28-491b-9b51-5111c395c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters with initial values\n",
    "total_reward = 0\n",
    "num_steps = 0\n",
    "done = False\n",
    "truncated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20cb8a6e-ab00-45cb-806c-b173ebfc5a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46586874,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resetting the state of the environment = resetting the starting point of the car\n",
    "# Initialize the environment\n",
    "observation, info = env.reset() # observation = the state\n",
    "# starting position has a random aspect\n",
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0014913c-72a4-4b4f-be06-300aefd9ea85",
   "metadata": {},
   "source": [
    "__Observation Space:__\n",
    "The observation is a ndarray with shape (2,) where the elements correspond to the following:\n",
    "<table align=\"left\" class=\"docutils align-default\">\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Num</p></th>\n",
    "<th class=\"head\"><p>Observation</p></th>\n",
    "<th class=\"head\"><p>Min</p></th>\n",
    "<th class=\"head\"><p>Max</p></th>\n",
    "<th class=\"head\"><p>Unit</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p>0</p></td>\n",
    "<td><p>position of the car along the x-axis</p></td>\n",
    "<td><p>-1.2</p></td>\n",
    "<td><p>0.6</p></td>\n",
    "<td><p>position (m)</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>1</p></td>\n",
    "<td><p>velocity of the car</p></td>\n",
    "<td><p>-0.07</p></td>\n",
    "<td><p>0.07</p></td>\n",
    "<td><p>velocity (v)</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8500098b-4e55-47bd-859b-9ebb520db6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a random action with the sample method: 0, 1 or 2\n",
    "action = env.action_space.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc7c92-e87c-44a7-8ade-978b9f511d5a",
   "metadata": {},
   "source": [
    "__Action Space:__ There are 3 discrete deterministic actions:\n",
    "\n",
    "0: Accelerate to the left\n",
    "\n",
    "1: Don’t accelerate\n",
    "\n",
    "2: Accelerate to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e1c53c6-c45f-4228-94be-4f46fa22f775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-4.6629956e-01, -4.3081396e-04], dtype=float32),\n",
       " -1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing the action to the step method to evaluate the next state\n",
    "# depending on the current state\n",
    "# truncated checks if we've reached the max episode steps\n",
    "observation, reward, done, truncated, info = env.step(action)\n",
    "observation, reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ca80d7-0592-46be-b90d-7d863218643c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update total rewards and the number of steps\n",
    "total_reward += reward\n",
    "num_steps += 1\n",
    "total_reward, num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c537a057-436d-42a0-98bb-c18124aad5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.6629956e-01, -4.3081396e-04], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the state\n",
    "state = observation\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09dfeb71-5466-417d-a8ac-a07490d1155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f675130e-6b98-425d-84e4-65dfa7abc7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access the internal step count\n",
    "num_steps = env._elapsed_steps\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842bc42-f379-469a-9733-ce6c9f1aa192",
   "metadata": {},
   "source": [
    "### Multiple random Episodes - with human rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb2dcb-a9d5-4cc9-b9c4-c2c80081a5f0",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">__Prompt:__ Now, I want to run multiple random episodes with human rendering. Please adjust the code accordingly with as few changes as possible.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bdd12-17a3-4f72-97ae-1eb48bc378d0",
   "metadata": {},
   "source": [
    "To run multiple random episodes with human rendering, you can simply add a loop around the episode logic. Here’s the adjusted code to run **multiple episodes**:\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "num_episodes = 5  # Set the number of episodes you want to run\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment at the beginning of each episode\n",
    "    observation, info = env.reset()\n",
    "    \n",
    "    total_reward = 0\n",
    "    num_steps = 0\n",
    "    done = False\n",
    "\n",
    "    print(f\"\\nStarting Episode {episode + 1}\")\n",
    "\n",
    "    # Run the episode until done\n",
    "    while not done:\n",
    "        # Take a random action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Perform the action in the environment\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # Accumulate the reward and count the steps\n",
    "        total_reward += reward\n",
    "        num_steps += 1\n",
    "\n",
    "    # Print the results for the current episode\n",
    "    print(f\"Episode {episode + 1} finished\")\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "    print(f\"Total Steps: {num_steps}\")\n",
    "\n",
    "# Close the environment after all episodes\n",
    "env.close()\n",
    "```\n",
    "\n",
    "### Changes:\n",
    "1. **Added a loop** around the episode logic to run multiple episodes (`for episode in range(num_episodes):`).\n",
    "2. Set `num_episodes` to control how many episodes you want to run (in this case, 5).\n",
    "3. A print statement was added before each episode to indicate which episode is starting.\n",
    "\n",
    "This code will now run 5 random episodes with human rendering enabled, showing the total reward and number of steps for each episode.\n",
    "\n",
    "Let me know if you need further adjustments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3be4c-665c-4026-b367-7c4665d10999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9995fd6f-6f2e-454b-a8f9-ac9da12eb8ad",
   "metadata": {},
   "source": [
    "### Performance Evaluation / Success Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707f1eb-cfa2-472d-be54-0e3ce2f51241",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">__Prompt:__ Now, I want to run multiple random episodes with human rendering. Please adjust the code accordingly with as few changes as possible.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218ae7f-bb91-4fa9-9428-b747b9cea473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faae01c8-8cd3-4021-8d4a-d8c084d5fb63",
   "metadata": {},
   "source": [
    "### Performance Evaluation / Success Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465b5d3-43a5-4d37-a3c0-011fcf1c000f",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">__Prompt:__ Now, let´s focus on Performance Measurement and Evaluation. I want to measure the performance / success of the random agent and print the performance at the end of the random run. Please make an appopriate proposal with various metrics. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d399270-24f1-4a75-b698-345f968a00b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b610490-eb95-4f30-88d7-ffc20ac53f16",
   "metadata": {},
   "source": [
    "### RGB Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c1d645-b431-4793-88a3-02d27995ace4",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">__Prompt:__ Human rendering is really slow. Please remove human rendering to run many episodes in no time. Print every 100th episode only.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f40ce9-cae5-4c65-a76a-7889cad1e6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "674ea282-0d21-4069-94c3-69002be867fa",
   "metadata": {},
   "source": [
    "### Training an RL Agent with Q-Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59cfcc-2f87-4473-be82-b3be99d06d20",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">__Prompt:__ I have the following code to run multiple random episodes of OpenAI´s MountainCar challenge: ---insert code---</span>\n",
    ">\n",
    "> <span style=\"color:blue\">Now, I want to create and train a Reinforcement Learning Agent that can reach the target with as few steps as possible and with a high success rate. Please build on the existing code and train the Agent. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0e6de-5c19-4c63-9586-0bef0d8868f4",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">__Prompt:__ XXX</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8b8ee-4773-4eb9-8566-2e9cdcbfd3a7",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">__Prompt:__ XXX</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36717cd5-16fc-4ab3-9bcb-3dad80433648",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">__Prompt:__ XXX</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
