{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa368979-5e97-4dec-a22e-b9777d1a2dbc",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4120c4d-81fd-4471-b958-17f95d1a710b",
   "metadata": {},
   "source": [
    "*[Documentinng a personal learning experience while working and coding with the printed book __Deep Reinforcement Learning with Python__: RLHF for Chatbots and Large Language Models by __Nimish Sanghi__, Apress, 2nd Edition July 2024, ISBN13 979-8868802720]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027c0a3-859e-436e-9335-170016501a40",
   "metadata": {},
   "source": [
    "__Reinforcement Learning from Human Feedback (RLHF):__ \n",
    "\n",
    "Applying deep reinforcement learning to Large Language Models (LLM) like ChatGPT to make the follow human instructions and produce output that is favored by humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0fd3a8-08bd-4d6d-8be3-81061a5edf22",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d3f7c-470b-4f65-a019-b4b314187de9",
   "metadata": {},
   "source": [
    "The book ***Deep Reinforcement Learning with Python*** by ***Nimish Sanghi*** is basically about __\"the theory and design of algorithms that help machines (agents) gain the capability to perform tasks by interacting with the environment and continuously learning from their successes, failures and rewards.\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd1b9a-eb28-4fa1-b371-93612f14c218",
   "metadata": {},
   "source": [
    "__Inference Engine in AI forerunners:__\n",
    "\n",
    "\"In expert systems (decision support systems) ... a knowledge base and an inference engine. A knowledge base is an organized collection of facts about the systemâ€™s domain. An inference engine interprets and evaluates the facts in the knowledge base in order to provide an answer. Typical tasks for expert systems involve classification, diagnosis, monitoring, design, scheduling, ...\" (https://www.britannica.com/technology/inference-engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9deaae-71f2-41fc-b4b9-8a38072cde0a",
   "metadata": {},
   "source": [
    "__The Modern Concept of Reinforcement Learning is a combination of two different threads:__\n",
    "\n",
    "1. __Concept of Optimal Control:__ \"Optimal control refers to solving problems where decisions are made continuously over time, considering changing information, by using the concept of 'cost-to-go' to minimize integrated costs from a current state to a target state.\" ([sciencedirect.com](https://www.sciencedirect.com/topics/social-sciences/optimal-control)) <br/>One approach to the problem of optimal control was Richard Bellman's discipline of *dynamic programming*, formalized in 1950. Bellman's dynamic programming is all about planning through the space of various options, using *Bellman recursive equations*.\n",
    "\n",
    "3. __Concept of Trial and Error:__ In 1910 Edward Thorndike formalised this theory into a *law* of psychology - *the law of effect*. In full it reads: \"Of several responses made to the same situation those which are accompanied or closely followed by satisfaction to the animal will, other things being equal, be more firmly connected with the situation, so that, when it recurs, they will be more likely to recur; those which are accompanied or closely followed by discomfort to the animal will, other things being equal, have their connections to the situation weakened, so that, when it recurs, they will be less likely to occur. The greater the satisfaction or discomfort, the greater the strengthening or weakening of the bond.\" ([faculty.sfcc.spokane.edu](https://faculty.sfcc.spokane.edu/InetShare/AutoWebs/KarlA/SPRING%202004/Ps101_Chap5_Article1.htm))\n",
    "\n",
    "In short, these concepts are about __increasing the occurence of good outcomes__ and __decreasing the occurence of bad outcomes__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac6b8c-2d4d-4f71-9657-36d688446224",
   "metadata": {},
   "source": [
    "## Core Elements of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ee7d7-1320-4d55-9a86-70e2a56be299",
   "metadata": {},
   "source": [
    "__The four key components of Reinforcement Learning:__\n",
    "\n",
    "1. Policy\n",
    "\n",
    "2. Rewards\n",
    "\n",
    "3. Value Functions\n",
    "\n",
    "4. Model of the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7901aab-eaa7-4867-9d5a-62fd4a783691",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24ff8e-bf4b-4b17-8308-c30e93fff587",
   "metadata": {},
   "source": [
    "__Policy__ maps the perceived state of the environment to the action that an agent takes. \n",
    "\n",
    "__Current State__: An agent is getting getting input from the system he's in (the environment), for instance a robot would collect observation data from the visual or other sensory input he gets. This input or data forms the *current state* of the environmnet.\n",
    "\n",
    "__Action:__ The agent (or robot) uses this information about the current state and past history if available, to decide what to do next (what *action* to perform next).\n",
    "\n",
    "The __policy__ maps the __state__ to the __action__ the agent takes. \n",
    "\n",
    "Policies can be \n",
    "- *deterministic* (for a given state of environment there is only one single fixed action)\n",
    "- or *stochastic* (for a given state of environment there are multiple possible actions)\n",
    "\n",
    "*(stochastic: having a random probability distribution or pattern that may be analysed statistically but may not be predicted precisely)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8511cab5-8c8a-459c-ac60-73b1cb8dab08",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9da13-bb30-47b1-92de-8994facd2e5a",
   "metadata": {},
   "source": [
    "__Reward__ refers to the goal/objective that the agent is trying to achieve. If an agent takes an action that brings it closer to its goal the reward should be positive. If the taken action takes the agent away from it's goal the outcome is unfavorable and the action should be considered to be negative. The *reward* is a numerical value that indicates the \"goodness\" of an action taken by the agent and therefore the primary way for the agent to assess if the taken action was good or bad. This information about the \"goodness\" of an action is used to adjust the behavior and by this optimizing the *policy* it is learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3f511-d697-42ff-92a0-ec74557b7898",
   "metadata": {},
   "source": [
    "### Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b552e-d26b-483c-a690-3f3191c6d268",
   "metadata": {},
   "source": [
    "__Value functions__ are defined by the policy the agent follows and long term-rewards.\n",
    "\n",
    "__Rewards__ are an intrinistic property of the environment. The current state the agent is in and the action it decides to take (the policy he follows) determine which reward will be obtained. Both taken together (rewards and policy) define the *value functions*.\n",
    "\n",
    "- __Value__ in a state is the total cumulative reward an agent is expected to get, based on *state* and *policy*.\n",
    "\n",
    "- __Rewards__ are *immediate feedback* from the environment based on the *state* and the *action* taken in that state.\n",
    "\n",
    "An agent accumulates rewards while following a policy and then uses these cumulative rewards to assess the value in a given state. As he has the goal to continiously increase the value of an observed state (the accumulated rewards) he makes changes to its policy with the goal to gain more rewards and thereby increase the value of the state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53061cae-6c38-4acb-b82f-5ace1883b92a",
   "metadata": {},
   "source": [
    "### Model of the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e2842-9d3a-47d2-86c7-a4c1e12a0e8c",
   "metadata": {},
   "source": [
    "__Model based Learning:__ To find optimal behavior agents use interactions with the environment to form an *internal model* of the environment. This internal model helps the agent to plan one or more chains of actions to achive his goal.\n",
    "\n",
    "__Model-free methods__ are completely based on trial and error and don't form any model of the environment.\n",
    "\n",
    "Agents usually use a combination of both methods to find the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db30a67-5c7d-4eb3-b820-e172c7685620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
