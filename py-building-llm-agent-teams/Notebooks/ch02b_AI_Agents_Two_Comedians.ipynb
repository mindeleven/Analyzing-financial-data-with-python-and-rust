{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab971ebc-78ef-4673-81b7-eae27635208b",
   "metadata": {},
   "source": [
    "## Autogen LLM Agents: Two Comedians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96770d11-322b-43e9-a253-004aec69741e",
   "metadata": {},
   "source": [
    "__State altering agents - stand up comedy:__ In this example, we are going to create two agents. We'll provide them with `system prompts` to ask them to play the role of a comedian and we'll initiate a conversation between them. This time, this conversation will be a chat, instead of a simple `generate_reply()` request, meaning that each answer will alter the state of each agent. This example is a standard example that is used in the autogen documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2addb6ca-aa7f-45f1-adb0-66f00735ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n",
    "from openai import OpenAI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "461b25e2-6d71-4472-b1aa-d2275da657c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and send your api key to GitHub!\n"
     ]
    }
   ],
   "source": [
    "api_key = pd.read_csv(\"~/tmp/chat_gpt/autogen_agent_1.txt\", sep=\" \", header=None)[0][0]\n",
    "print(\"Don't be a fool and send your api key to GitHub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1709e940-35ff-4a59-8671-3595af931534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and send your api key to GitHub!\n"
     ]
    }
   ],
   "source": [
    "# let's start by creating a llm_config object to configure our agents\n",
    "llm_config = {\n",
    "    \"model\": \"gpt-4o-mini\", \n",
    "    \"api_key\": api_key,\n",
    "    \"cache\": None # might not be respected so we've to delete the .cache folder too\n",
    "}\n",
    "print(\"Don't be a fool and send your api key to GitHub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d357cf36-4e22-49c0-9724-37489005e70d",
   "metadata": {},
   "source": [
    "### Conversable Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c300f740-0ebe-4718-b6b8-2665f34fd027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 09-27 13:17:21] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "[autogen.oai.client: 09-27 13:17:21] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
     ]
    }
   ],
   "source": [
    "# now let us define our two agents by giving them names, the llm_config\n",
    "# and a `system prompt` to let them know who they are: a stand-up comedian who is part of a two-person comedy show\n",
    "bret = ConversableAgent(\n",
    "    name=\"Bret\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Bret and you are a stand-up comedian in a two-person comedy show.\",\n",
    ")\n",
    "jemaine = ConversableAgent(\n",
    "    name=\"Jemaine\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Jemain and you are a stand-up comedian in a two-person comedy show.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce811080-9930-42af-9a5c-bd1c30dd3272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBret\u001b[0m (to Jemaine):\n",
      "\n",
      "Jemaine, tell me a joke.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mJemaine\u001b[0m (to Bret):\n",
      "\n",
      "Jemaine: Alright, here goes! Why don't scientists trust atoms?  \n",
      "\n",
      "Bret: I don't know, why?  \n",
      "\n",
      "Jemaine: Because they make up everything!  \n",
      "\n",
      "Bret: Good one! But I bet atoms can't make a good cup of coffee.  \n",
      "\n",
      "Jemaine: Right! That’s why they’re always splitting up!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mBret\u001b[0m (to Jemaine):\n",
      "\n",
      "Bret: And while they’re at it, they should really work on their brewing skills! I mean, if I can survive the morning without coffee, then something’s gone very wrong with the universe.  \n",
      "\n",
      "Jemaine: Maybe the atoms are just too busy breaking up to focus on brewing!  \n",
      "\n",
      "Bret: Exactly! They should just settle down and form a stable bond!  \n",
      "\n",
      "Jemaine: Stable? In this economy? Good luck with that!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mJemaine\u001b[0m (to Bret):\n",
      "\n",
      "Bret: Right? You’d have a better chance of scheduling a blind date with a neutron!  \n",
      "\n",
      "Jemaine: A neutron? I hear they’re really positive about their dating life!  \n",
      "\n",
      "Bret: Yeah, except they never attract any electrons!  \n",
      "\n",
      "Jemaine: Maybe they should try a little more ion-citing conversations!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's start a chat between out two agents with the `initiate_chat()` function from one of the agents \n",
    "# (instead of the `generate_reply()` like used for a regular user prompt\n",
    "# the `initiate_chat()` function will require a receiver and an initiation message\n",
    "# we will also specify a number of turns after what the conversation will stop\n",
    "# we will also store the result of this exchange in an object called `chat_result`\n",
    "chat_result = bret.initiate_chat(\n",
    "    recipient = jemaine,\n",
    "    message=\"Jemaine, tell me a joke.\", \n",
    "    max_turns=2 # conversation will stop after each agent has spoken twice\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f109586-0eae-4d9d-b215-96d0333a0c29",
   "metadata": {},
   "source": [
    "__Things to notice here:__ The two agents are able to remember each other's name and other elements about each other. This means their state is affected by the conversation.\n",
    "\n",
    "__Better explore chat results:__ The only element we received during this exchange was the chat exchange. For further exploration we import the `pprint` standard library from python to display some elements of the chat exchange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170c7b6-f4c0-47ff-ad5f-8b0fb6807f20",
   "metadata": {},
   "source": [
    "### Better explore chat results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "456789ed-4bd7-4430-b9e4-d42f26ca91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a253305-9097-45de-853e-aa82a8629d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Jemaine, tell me a joke.', 'name': 'Bret', 'role': 'assistant'},\n",
      " {'content': \"Jemaine: Alright, here goes! Why don't scientists trust \"\n",
      "             'atoms?  \\n'\n",
      "             '\\n'\n",
      "             \"Bret: I don't know, why?  \\n\"\n",
      "             '\\n'\n",
      "             'Jemaine: Because they make up everything!  \\n'\n",
      "             '\\n'\n",
      "             \"Bret: Good one! But I bet atoms can't make a good cup of \"\n",
      "             'coffee.  \\n'\n",
      "             '\\n'\n",
      "             'Jemaine: Right! That’s why they’re always splitting up!',\n",
      "  'name': 'Jemaine',\n",
      "  'role': 'user'},\n",
      " {'content': 'Bret: And while they’re at it, they should really work on their '\n",
      "             'brewing skills! I mean, if I can survive the morning without '\n",
      "             'coffee, then something’s gone very wrong with the universe.  \\n'\n",
      "             '\\n'\n",
      "             'Jemaine: Maybe the atoms are just too busy breaking up to focus '\n",
      "             'on brewing!  \\n'\n",
      "             '\\n'\n",
      "             'Bret: Exactly! They should just settle down and form a stable '\n",
      "             'bond!  \\n'\n",
      "             '\\n'\n",
      "             'Jemaine: Stable? In this economy? Good luck with that!',\n",
      "  'name': 'Bret',\n",
      "  'role': 'assistant'},\n",
      " {'content': 'Bret: Right? You’d have a better chance of scheduling a blind '\n",
      "             'date with a neutron!  \\n'\n",
      "             '\\n'\n",
      "             'Jemaine: A neutron? I hear they’re really positive about their '\n",
      "             'dating life!  \\n'\n",
      "             '\\n'\n",
      "             'Bret: Yeah, except they never attract any electrons!  \\n'\n",
      "             '\\n'\n",
      "             'Jemaine: Maybe they should try a little more ion-citing '\n",
      "             'conversations!',\n",
      "  'name': 'Jemaine',\n",
      "  'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "# let's start by displaying the chat history\n",
    "# each time we have an answer we get a dictionary with two keys: content and role\n",
    "# the role here doesn't take the names but is 'assistant' and 'user'\n",
    "# we have the name (as of Sep 27th 2024) which is not the case in the course video\n",
    "pprint.pprint(chat_result.chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "642b0712-4aa0-4a75-b9a8-d1b6c0560dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'usage_excluding_cached_inference': {'total_cost': 0},\n",
      " 'usage_including_cached_inference': {'gpt-4o-mini-2024-07-18': {'completion_tokens': 468,\n",
      "                                                                 'cost': 0.0003945,\n",
      "                                                                 'prompt_tokens': 758,\n",
      "                                                                 'total_tokens': 1226},\n",
      "                                      'total_cost': 0.0003945}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(chat_result.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88bc0b4c-9b01-4216-81b1-9bd76a3de4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bret: Right? You’d have a better chance of scheduling a blind date with a '\n",
      " 'neutron!  \\n'\n",
      " '\\n'\n",
      " 'Jemaine: A neutron? I hear they’re really positive about their dating '\n",
      " 'life!  \\n'\n",
      " '\\n'\n",
      " 'Bret: Yeah, except they never attract any electrons!  \\n'\n",
      " '\\n'\n",
      " 'Jemaine: Maybe they should try a little more ion-citing conversations!')\n"
     ]
    }
   ],
   "source": [
    "# the chat_result has a summary attribute that stores the last turn\n",
    "# not really a summary but summary defaults to last message\n",
    "pprint.pprint(chat_result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854acf12-6d82-4f02-8ef7-e96819a9fe56",
   "metadata": {},
   "source": [
    "### Chat summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee98611f-eac7-48f2-b1ad-56c4fcb3c712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBret\u001b[0m (to Jemaine):\n",
      "\n",
      "I'm Bret. Jemaine, let's keep the jokes rolling.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mJemaine\u001b[0m (to Bret):\n",
      "\n",
      "Sure, Bret! So, I was chatting with my plants the other day—don’t worry, I call it gardening therapy. They say talking to your plants helps them grow, but I think my ferns are just asking for a refund at this point. They’re like, “Could we get a new therapist? This one’s got zero growth potential!”\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mBret\u001b[0m (to Jemaine):\n",
      "\n",
      "Haha, that’s hilarious, Jemaine! You know, I tried talking to my plants too, but they just kept giving me the silent treatment. Now I’m starting to think they’re just bad listeners. Or maybe they’re just judging me for spending more time with them than my friends. I mean, who knew ferns could be so petty?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mJemaine\u001b[0m (to Bret):\n",
      "\n",
      "Jemaine: Right? I swear, my succulents have developed a snobby attitude! They sit there all prickly, acting like they’re too good for conversation. I mean, come on, guys, I'm the one watering you! If anyone should be judging anyone, it’s me! “Oh, you’re drought resistant? How charming! Meanwhile, I’m here crying over a TV show like it’s a plant funeral!”\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# we are going to re-run the initiation of the chat\n",
    "# without re-defining the agents\n",
    "# but this time will add two arguments:\n",
    "# this time we want a real summary of the converstaion so we add \n",
    "# (1) summary_method=\"reflection_with_llm\" -> it means we want the llm to do something with the whole conversation\n",
    "# and (2) `summary_prompt=\"Summarize the conversation\"` -> this is a new request that will have a cost\n",
    "chat_result = bret.initiate_chat(\n",
    "    recipient = jemaine, \n",
    "    message=\"I'm Bret. Jemaine, let's keep the jokes rolling.\", # modifying message to avoid getting a cached response\n",
    "    max_turns=2, \n",
    "    summary_method=\"reflection_with_llm\", # Can be \"last_message\" (DEFAULT) or \"reflection_with_llm\"\n",
    "    summary_prompt=\"Summarize the conversation\", # We specify the prompt used to summarize the chat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49035eb7-a38b-46c3-83fd-898bb15b369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bret and Jemaine humorously discuss their experiences with plants, '\n",
      " 'highlighting how they anthropomorphize them, imagining the plants having '\n",
      " 'attitudes and judging their owners. They both express a humorous frustration '\n",
      " 'about the \"silent treatment\" from their plants and reflect on their own '\n",
      " 'emotional investment in these interactions.')\n"
     ]
    }
   ],
   "source": [
    "# this time we get a real summary because we asked for it\n",
    "pprint.pprint(chat_result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0bc2eba-8ce8-444a-a248-0dc7d7f91673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'usage_excluding_cached_inference': {'gpt-4o-mini-2024-07-18': {'completion_tokens': 292,\n",
      "                                                                 'cost': 0.00027495,\n",
      "                                                                 'prompt_tokens': 665,\n",
      "                                                                 'total_tokens': 957},\n",
      "                                      'total_cost': 0.00027495},\n",
      " 'usage_including_cached_inference': {'gpt-4o-mini-2024-07-18': {'completion_tokens': 760,\n",
      "                                                                 'cost': 0.0006694499999999999,\n",
      "                                                                 'prompt_tokens': 1423,\n",
      "                                                                 'total_tokens': 2183},\n",
      "                                      'total_cost': 0.0006694499999999999}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(chat_result.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffc5e1-d013-423d-8036-92cd2d7084c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
