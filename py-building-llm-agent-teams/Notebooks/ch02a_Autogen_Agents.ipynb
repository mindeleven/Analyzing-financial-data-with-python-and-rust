{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e3ed12-0269-494c-9bea-9a844ea12d67",
   "metadata": {},
   "source": [
    "## Autogen Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf010a7-ee81-44a0-91fb-545ec729e3bb",
   "metadata": {},
   "source": [
    "*[Coding along with the Udemy online course AI Agents: Building Teams of LLM Agents that Work For You by Mohsen Hassan & Ilyass Tabiai]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96a586-6fb2-405d-82f9-d821f9f19087",
   "metadata": {},
   "source": [
    "**AutoGen is an open-source framework that leverages multiple agents to enable complex workflows.**\n",
    "\n",
    "- Autogen was developed by Microsoft to create advanced LLM-based applications.\n",
    "- Autogen is designed to enable you to orchestrate multi-agent conversations that seamlessly integrate LLMs, human input and other tools.\n",
    "- Autogen is supported by Microsoft, which gives it long term viability and was designed to specifically work with OpenAI, so it will be the framework we will be using in this class for agentic design.\n",
    "\n",
    "**An online tutorial can be found at [Getting Started with AutoGen](https://microsoft.github.io/autogen/docs/Getting-Started).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7baed5-7848-4482-afab-5d61d2921925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58874505-b0f8-480d-b8d2-20bc6dd2c5aa",
   "metadata": {},
   "source": [
    "Importing `autogen` returned the following error:<br/>\n",
    "`flaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.`\n",
    "\n",
    "The temporary solution to solve this was to create a constraints.txt file with the following content:\n",
    "\n",
    "`flaml==2.2.0`\n",
    "\n",
    "Then running: `pip install -c constraints.txt autogen`\n",
    "\n",
    "Following the suggestion at https://github.com/microsoft/autogen/issues/3548.\n",
    "\n",
    "*FLAML @ https://microsoft.github.io/FLAML/ is \"A Fast Library for Automated Machine Learning & Tuning.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1f800-24f9-4f24-9992-9777a78fd10c",
   "metadata": {},
   "source": [
    "<img decoding=\"async\" loading=\"lazy\" alt=\"AutoGen Overview\" src=\"../assets/images/microsoft-autogen-1.png\" width=\"75%\">\n",
    "<br/><i>Screenshot from Getting Started with AutoGen @ https://microsoft.github.io/autogen/docs/Getting-Started</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc15a7-22c8-45d4-ab7c-160ad7899682",
   "metadata": {},
   "source": [
    "## Conversable Agent\n",
    "\n",
    "* AutoGen offers a unified multi-agent conversation framework that simplifies the orchestration and automation of complex LLM workflows the Conversable Agent class. These agents can collectively perform tasks autonomously or with human feedback.\n",
    "* Conversable Agents are:\n",
    "    * **Conversable**: Agents can send and receive messages from other agents or humans; they can initate and continue a conversation seamlessly.\n",
    "    * **Customizable**: They can be customized to integrate LLMs, human intervention, tools/skills (code) or combinations of these.\n",
    "    * Of two subcategories:\n",
    "        * **AssistantAgent**: This subcategory represents AI assistants, that use LLMs (such as GPT-4) by default. It can generate Python code for users to execute based on task descriptions. Additionally, it can receive execution results (from code or other agents) and suggest corrections or bug fixes.\n",
    "        * **UserProxyAgent**:  This subcategory acts as a proxy agent for humans. It solicits human input as its reply during interactions. It also has the capability to execute code and call functions or tools. Code execution can be automatically triggered when an executable code block is detected in the received message.\n",
    "\n",
    "Let's create a Conversable Agent to accomlpish a simple task. Before doing that, we will have to create a LLM config that specifies which LLM we want to use. Here we will again use chatGPT3.5 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1145aa7e-8881-4bfe-be34-67fe600fc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "066913e2-3039-49d0-82e0-5422c14fbec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and send your api key to GitHub!\n"
     ]
    }
   ],
   "source": [
    "api_key = pd.read_csv(\"~/tmp/chat_gpt/gippity_key_1.txt\", sep=\" \", header=None)[0][0]\n",
    "print(\"Don't be a fool and send your api key to GitHub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d708969c-6210-422f-9876-cf44de9977f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model from the list of models provided by OpenAI https://platform.openai.com/docs/models/continuous-model-upgrades\n",
    "llm_config = {\n",
    "    # \"model\": \"gpt-4o-mini\",\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"api_key\": api_key\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754d5c5-112c-4013-9524-77dc0f34679b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bab46ad-c2b1-413e-a8ff-42d0a2a66e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ConversableAgent(\n",
    "    name=\"chatbot\",\n",
    "    llm_config=llm_config, # The Agent will use the LLM config provided to answer\n",
    "    human_input_mode=\"NEVER\", # Can also be ALWAYS or TERMINATE (at end only)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1e9f7-c9c6-4549-a228-ca21ee1c071c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
