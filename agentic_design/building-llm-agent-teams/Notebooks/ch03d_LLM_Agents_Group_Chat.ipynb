{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4028ba8d-daff-4217-9c54-ea60a85b5b1c",
   "metadata": {},
   "source": [
    "## LLM Agents Group Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3d6cd-10a5-45e5-9c0b-b36d9a5e6026",
   "metadata": {},
   "source": [
    "*[Coding along with the Udemy online course AI Agents: Building Teams of LLM Agents that Work For You by Mohsen Hassan & Ilyass Tabiai]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2841d0-3556-40ba-b071-c1c2769a7f99",
   "metadata": {},
   "source": [
    "### Teamwork: Planning a stock report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375d627-584d-4a7d-a89f-d1602d17fca2",
   "metadata": {},
   "source": [
    "What we're going to do in this exercise is to setup a team of agents and let them determine how to accomplish the task by themsleves. \n",
    "\n",
    "It's a less efficient method than the ones we've previously used and at the same time it's the one that requires the least of involvement on the user's side.\n",
    "\n",
    "In order for the team to be able to work together, we will provide them with a __Planner agent__. The Planner agent's role will be \n",
    "- to determine which task should be done first\n",
    "- and which agent should accomplish it.\n",
    "\n",
    "This means that we should something to the agent's definitions to let other agents know about their role, so that the Planner can determine which agent can do which task.\n",
    "\n",
    "__The task we are going to focus on will be writing a blogpost about the stock performance of a specific asset.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266fbd81-632d-4d0d-a7fa-b3405ff7e561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't be a fool and send your api key to GitHub!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "api_key = pd.read_csv(\"~/tmp/chat_gpt/autogen_agent_1.txt\", sep=\" \", header=None)[0][0]\n",
    "\n",
    "llm_config = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"api_key\": api_key\n",
    "    }\n",
    "\n",
    "print(\"Don't be a fool and send your api key to GitHub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c3cea4-8e1c-4c82-bc2f-863c9ee49c12",
   "metadata": {},
   "source": [
    "### Team description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc071d-61f1-48aa-9900-95eccd983dd8",
   "metadata": {},
   "source": [
    "* **User_proxy or Admin**: to allow the user to comment on the report and ask the writer to refine it.\n",
    "* **Planner**: to determine relevant information needed to complete the task.\n",
    "* **Engineer**: to write code using the defined plan by the planner.\n",
    "* **Executor**: to execute the code written by the engineer.\n",
    "* **Writer**: to write the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a9591-1799-4dbf-bbee-267571bbc989",
   "metadata": {},
   "source": [
    "### The task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d953df6-59f4-4c5d-93d6-c387e81a6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Write a blogpost about the stock price performance of \"\\\n",
    "\"Nvidia in the past month. Today's date is 2024-10-01.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cdcfaa-e601-4b4c-b2c8-33bbf557f408",
   "metadata": {},
   "source": [
    "## Defining our agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5f4116-fc9c-4856-8f57-a216eef42785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217029a8-ef4e-4396-ae86-0b2ba48df5ca",
   "metadata": {},
   "source": [
    "### User Proxy Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb7863-3e6d-41b0-9806-36b602574f93",
   "metadata": {},
   "source": [
    "The __User Proxy Agent__ will allow us to interact with the agents. It can use auto-reply, so it will have __access to an LLM config__, but it will also have its __Human Input Mode to Always__, which will allow us to interact with the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1de211f-8a32-46fd-8b55-5f631187650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 10-07 12:52:11] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogen.agentchat.conversable_agent.ConversableAgent at 0x1483e1c40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_proxy = autogen.ConversableAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"Give the task, and send \"\n",
    "    \"instructions to writer to refine the blog post.\",\n",
    "    code_execution_config=False,\n",
    "    llm_config=llm_config, \n",
    "    human_input_mode=\"ALWAYS\", # If user does not provide feedback, the LLM will do it for us\n",
    ")\n",
    "user_proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86849829-84d1-4bb9-b685-a96972d8a213",
   "metadata": {},
   "source": [
    "### Planner Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f80b6-ebda-4e1f-81a5-ea65ef319d41",
   "metadata": {},
   "source": [
    "It is possible to add a **description message** to an agent, which is known by other agents and is meant to make tham understand this agent's role. This way other agents, and specifically the Planner, can determine which agent can accomplish which task.\n",
    "\n",
    "The **system message prompt** that we used so far is *only known to each agent* and defines how they should behave.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78fcdf6d-2356-4613-86da-e1dc549bc617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 10-07 13:08:54] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
     ]
    }
   ],
   "source": [
    "# defining a Planner Agent with a description\n",
    "planner = autogen.ConversableAgent(\n",
    "    name=\"Planner\",\n",
    "    system_message=\"Given a task, please determine \"\n",
    "    \"what information is needed to complete the task. \"\n",
    "    \"Please note that the information will all be retrieved using\"\n",
    "    \" Python code. Please only suggest information that can be \"\n",
    "    \"retrieved using Python code. \"\n",
    "    \"After each step is done by others, check the progress and \"\n",
    "    \"instruct the remaining steps. If a step fails, try to \"\n",
    "    \"workaround\",\n",
    "    description=\"Planner. Given a task, determine what \"\n",
    "    \"information is needed to complete the task. \"\n",
    "    \"After each step is done by others, check the progress and \"\n",
    "    \"instruct the remaining steps\",\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d5d38-4a76-43cf-b165-496f06a20fbc",
   "metadata": {},
   "source": [
    "### Engineer Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8841193-b50c-42d8-87a7-62d0f3ee4a5b",
   "metadata": {},
   "source": [
    "The Engineer agent already has a __default system prompt__, which is enough for the task we would like it to accomplish, write code.  \n",
    "We are going to add a description message that lets other agents know about its role. Other agents will also know about the name of agents, and in somes cases that might be enough to define their roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b414d7eb-a33f-4a8e-aa8f-0b9536b2345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 10-07 13:21:07] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "You are a helpful AI assistant.\n",
      "Solve tasks using your coding and language skills.\n",
      "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
      "    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
      "    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\n",
      "Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\n",
      "When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\n",
      "If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\n",
      "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
      "When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.\n",
      "Reply \"TERMINATE\" in the end when everything is done.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# system msg: Already has a default one because it is a code writer\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=llm_config,\n",
    "    description=\"An engineer that writes code based on the plan \"\n",
    "    \"provided by the planner.\",\n",
    ")\n",
    "\n",
    "# Check engineer system prompt message\n",
    "print(engineer.system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f8d381-1f24-4259-a42a-6c8912ece7fc",
   "metadata": {},
   "source": [
    "### Executor Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181bdb5-073f-4642-b798-3952f0a69df1",
   "metadata": {},
   "source": [
    "The Executor agent is a __code executor similar to the ones we already used__. Since this time there will be several agents exchanging through a group chat and we're unsure in which order that'll happen, we are going to add an argument in the `code_execution_config`, a parameter that specifies that __we want this agent to know about the last 3 messages instead of the last one only__.\n",
    "\n",
    "__The name of the agent in this case is enough to share its role with other agents.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd1235a-fdba-401a-a458-db4f6f36ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = autogen.ConversableAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Execute the code written by the \"\n",
    "    \"engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f89cc3-ee84-4625-92a2-4637fece1369",
   "metadata": {},
   "source": [
    "### Writer Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b58ece0-3e86-43d5-94f3-ac25820a97ba",
   "metadata": {},
   "source": [
    "This agent is the writer agent who is going to write the report we want as a blogpost. This agent will have a system message and a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ac1c8e-7789-48ce-9aa5-b5f741fbaa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 10-07 13:29:43] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
     ]
    }
   ],
   "source": [
    "writer = autogen.ConversableAgent(\n",
    "    name=\"Writer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Writer.\"\n",
    "    \"Please write blogs in markdown format (with relevant titles)\"\n",
    "    \" and put the content in pseudo ```md``` code block. \"\n",
    "    \"You take feedback from the admin and refine your blog.\",\n",
    "    description=\"Writer.\"\n",
    "    \"Write blogs based on the code execution results and take \"\n",
    "    \"feedback from the admin to refine the blog.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38670c5-aad8-48bf-a79d-f03afc7cb24d",
   "metadata": {},
   "source": [
    "### The Group Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d635cc0-b076-4065-a8a6-af2134f2994f",
   "metadata": {},
   "source": [
    "We are now going to define the chat, this time, it's going to be a __Group Chat__. This means __we won't define ay conversation order__ other than specifying who is the Manager. We'll just list the agents who'll be taken part in this group chat, and the max number of turns  we allow for it to last:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8d720c5-1e87-4f56-ad74-df1f36353596",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, writer, executor, planner],\n",
    "    messages=[], # No initial message\n",
    "    max_round=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99946acb-9b1f-44ab-b16d-b917110a8db5",
   "metadata": {},
   "source": [
    "We're also going to provide this group chat with a __Group Chat Manager__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffdb1995-83e3-4fed-9a47-b9b94da55d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 10-07 13:32:14] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
     ]
    }
   ],
   "source": [
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat, \n",
    "    llm_config=llm_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54550120-d3ea-4d93-9026-ee52c9ec08c1",
   "metadata": {},
   "source": [
    "### Chat Initiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63bed4-9bcd-4206-8abb-d989cdaa6ec2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's now initiate the chat with a message from the User Proxy agent (which is us) to the manager, giving him the task we previously defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c17f6e60-ef99-43e2-b2c2-20c3f7ea3c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# code that would execute our task but it's not supposed to run inside a notebook :)\\ngroupchat_result = user_proxy.initiate_chat(\\n    manager, # We initiate between user and manager so we can give the task\\n    message=task,\\n)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# code that would execute our task but it's not supposed to run inside a notebook :)\n",
    "groupchat_result = user_proxy.initiate_chat(\n",
    "    manager, # We initiate between user and manager so we can give the task\n",
    "    message=task,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6ffcc-c64b-418e-8e78-429572995744",
   "metadata": {},
   "source": [
    "# Group Chat Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f63a9-3f02-4dd2-9c31-5e4acf358186",
   "metadata": {},
   "source": [
    "__A first run of the Group Chat produced the following result:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2c8c2-8725-4488-98a4-e752cf5b7dce",
   "metadata": {},
   "source": [
    "# Nvidia Stock Performance in September 2024: A Statistical Overview with Market Insights\n",
    "\n",
    "As October rolls in, it's essential to look back and examine Nvidia's (NVDA) stock market performance over September 2024. This month saw vibrant market activity, marked by notable shifts and encouraging growth for one of the tech sector’s foremost innovators. Let's delve into Nvidia's market trends, supported by visual data and contextual analysis.\n",
    "\n",
    "## Monthly Statistical Highlights\n",
    "\n",
    "Starting at an opening price of $116.01 on September 1st, Nvidia's stock climbed to a closing price of $121.40 on September 30th, reflecting a moderate overall gain of around 4.65%. Here's a summary of significant monthly statistics:\n",
    "\n",
    "- **Opening Price (Sep 1, 2024):** $116.01\n",
    "\n",
    "- **Closing Price (Sep 30, 2024):** $121.40\n",
    "\n",
    "- **Highest Price in September:** $127.67 (recorded on Sep 26)\n",
    "\n",
    "- **Lowest Price in September:** $100.95 (recorded on Sep 6)\n",
    "\n",
    "- **Average Closing Price:** $114.72<br/>\n",
    "\n",
    "## Stock Price Visualization\n",
    "\n",
    "Below is a plot reflecting Nvidia's daily closing prices throughout September 2024. The chart highlights the highest and lowest points of the month, providing a clear depiction of major price fluctuations and market trends.\n",
    "\n",
    "![Nvidia Stock Price in September 2024](../coding/nvidia_stock_price_september_2024.png)\n",
    "\n",
    "## Analyzing Key Market Movements\n",
    "\n",
    "Significant price movements were observed on specific dates, driven by impactful news and market dynamics:\n",
    "\n",
    "**Significant Rise on September 11:**\n",
    "- Nvidia's stock saw an impressive spike of 8.15% on this date. This increase followed the announcement of key developments in their graphics card lineup, enhancing gaming computing capabilities, which generated widespread investor enthusiasm. According to *[TechCrunch]*, the announcement highlighted Nvidia's push into leveraging AI for graphics transformations, reinforcing market confidence.\n",
    "\n",
    "**Sharp Decline on September 6:**\n",
    "- Conversely, the steep price drop of 4.09% observed on September 6 can be linked to broader market anxieties over tech stock valuations following the latest US Federal Reserve interest rate decision. *[Bloomberg]* reported industry-wide effects that influenced sentiment across numerous tech stocks, including Nvidia.\n",
    "\n",
    "## Conclusion and Forward Outlook\n",
    "\n",
    "Nvidia demonstrated remarkable resilience amidst market fluctuations, closing September on a high note. The notable highs and lows present opportunities for agile investors and underscore Nvidia's dynamic market presence. Looking ahead, the focus will be on Nvidia’s strategic initiatives and sector movements. Expectations are high in anticipation of developments such as further advancements in AI and deep learning applications.\n",
    "\n",
    "Stay tuned for our upcoming monthly analyses, where we'll continue to monitor Nvidia’s performance and dissect the factors shaping its market journey.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f534e435-180b-4035-af11-50988e345fd3",
   "metadata": {},
   "source": [
    "# Group Chat with constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e197bc-0174-4ca8-9742-ee19024e328e",
   "metadata": {},
   "source": [
    "In some cases, you might want to constraint exchanges within the chat. That measn that you might want to stop some agents from immediately interacting with other agents or the opposite. You can do so by adding an argument to your Group Chat definition.\n",
    "\n",
    "For example, if we want to ensure that the engineer will only interact with the **Executor** or **User proxy** (us), we can specify that.  \n",
    "Here is how we would define such constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1e3b942-9b75-4cda-8f95-8d9a7a43c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, writer, executor, planner],\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    allowed_or_disallowed_speaker_transitions={\n",
    "        user_proxy: [engineer, writer, executor, planner],\n",
    "        engineer: [user_proxy, executor],\n",
    "        writer: [user_proxy, planner],\n",
    "        executor: [user_proxy, engineer, planner],\n",
    "        planner: [user_proxy, engineer, writer],\n",
    "    },\n",
    "    speaker_transitions_type=\"allowed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f44d1-6d95-4dc8-b05d-c3c27b7175c3",
   "metadata": {},
   "source": [
    "We can then simply re-start the chat with these constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9ec5d83-2aa2-4895-a32f-b2facac34130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 10-07 15:13:29] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# code that would execute our task but it's not supposed to run inside a notebook :)\\ngroupchat_result = user_proxy.initiate_chat(\\n    manager,\\n    message=task,\\n)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat, llm_config=llm_config\n",
    ")\n",
    "\n",
    "'''\n",
    "# code that would execute our task but it's not supposed to run inside a notebook :)\n",
    "groupchat_result = user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=task,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719409f-ac1a-480c-9ac4-7187432b6b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
